# âš ï¸ EXAMPLE WORKFLOW - REQUIRES INTEGRATION
# This workflow was exported from a working project and serves as an EXAMPLE ONLY.
# You MUST adapt the following before using:
# - $GCP_PROJECT_ID â†’ Your Google Cloud project ID
# - $GITHUB_REPOSITORY â†’ Your repository (owner/repo)
# - $GITHUB_OWNER â†’ Your GitHub username or org
# - Service account configurations and secrets
# - Environment-specific variables and paths
#
# See workflows/README.md for integration instructions.
# ---

name: WorldArchitect Tests (Directory-Based)

# Cancel in-progress runs when new commits are pushed to same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Security permissions for directory-based testing
permissions:
  contents: read
  pull-requests: read

# When to run the tests (optimized - no duplication on PR branches)
on:
  workflow_dispatch:
  pull_request:              # Run on all PRs regardless of target branch
    paths-ignore:
      - '**/*.md'
      - 'docs/**'
      - 'roadmap/**'
      - '.github/ISSUE_TEMPLATE/**'
  push:
    branches: [ main, dev ]  # Run on pushes to main/dev for post-merge validation
    paths-ignore:
      - '**/*.md'
      - 'docs/**'
      - 'roadmap/**'
      - '.github/ISSUE_TEMPLATE/**'

jobs:
  limit-pr-runs:
    if: github.event_name == 'pull_request' && github.event.pull_request.head.repo.fork == false
    runs-on: ubuntu-latest
    permissions:
      actions: write
      contents: read
      pull-requests: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4.3.1 # v4.1.1
      - name: Limit PR runs
        uses: ./.github/actions/limit-pr-runs
        with:
          gh_token: ${{ secrets.GITHUB_TOKEN }}
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      test-dirs: ${{ steps.changes.outputs.test-dirs }}
      selected-groups: ${{ steps.changes.outputs.selected-groups }}
      matrix: ${{ steps.changes.outputs.matrix }}
      has-changes: ${{ steps.changes.outputs.has-changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4.3.1  # v4.1.1
        with:
          fetch-depth: 0  # Full history for accurate change detection

      - name: Detect directory changes
        id: changes
        run: |
          # Use shared change detection script
          chmod +x scripts/ci-detect-changes.sh

          # Run change detection and capture output
          OUTPUT=$(./scripts/ci-detect-changes.sh \
            "${{ github.event_name }}" \
            "${{ github.event.pull_request.base.sha }}" \
            "${{ github.event.pull_request.head.sha }}" \
            "include")

          # Parse and export outputs
          echo "$OUTPUT" | while IFS= read -r line; do
            if [[ "$line" =~ ^test-dirs= ]]; then
              echo "$line" >> $GITHUB_OUTPUT
            elif [[ "$line" =~ ^selected-groups= ]]; then
              echo "$line" >> $GITHUB_OUTPUT
            elif [[ "$line" =~ ^matrix= ]]; then
              echo "$line" >> $GITHUB_OUTPUT
            elif [[ "$line" =~ ^has-changes= ]]; then
              echo "$line" >> $GITHUB_OUTPUT
            fi
          done

          echo "$OUTPUT"

  # Import validation (always runs)
  import-validation:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4.3.1  # v4.1.1
        with:
          fetch-depth: 0  # Full history needed for merge-base diff
      - name: Run import validation
        run: |
          if [ -f "scripts/validate_imports_delta.sh" ]; then
            ./scripts/validate_imports_delta.sh
          else
            echo "Import validation script not found, skipping"
          fi
      - name: Upload import validation results
        uses: actions/upload-artifact@5d5d22a31266ced268874388b861e4b58bb5c2f3  # v4.3.1
        if: always()
        with:
          name: import-validation-results
          path: scripts/validate_imports.log

  # Merge-commit correctness gate (REV-q4cmy)
  # The testmon test jobs checkout PR head SHA for cache stability,
  # but that skips validation against the merge commit (base+head).
  # This lightweight gate runs on the default merge commit to catch
  # integration breakages that would only appear after merging.
  merge-commit-gate:
    name: Merge commit validation
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    env:
      TESTING: 'true'
      TESTING_AUTH_BYPASS: 'true'
      MOCK_SERVICES_MODE: 'true'
      FAST_TESTS: '1'
      GEMINI_API_KEY: test-ci-key-only
      CEREBRAS_API_KEY: test-ci-key-only
      OPENROUTER_API_KEY: test-ci-key-only
      GOOGLE_APPLICATION_CREDENTIALS: /dev/null
      ENABLE_SEMANTIC_ROUTING: 'false'
      PYTHONPATH: ${{ github.workspace }}:${{ github.workspace }}/mvp_site:${{ github.workspace }}/automation
    steps:
    - name: Checkout merge commit
      uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4.3.1  # v4.1.1
      with:
        # Deliberately NOT setting ref â€” uses default merge commit
        # to validate the PR integrates cleanly with the base branch.
        submodules: recursive
        fetch-depth: 1

    - name: Set up Python 3.11
      uses: actions/setup-python@0a5c61591373683505ea898e09a3ea4f39ef2b9c  # v5.0.0
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f mvp_site/requirements.txt ]; then pip install -r mvp_site/requirements.txt; fi

    - name: Validate merge-commit imports
      run: |
        echo "=== Merge-commit import validation ==="
        echo "Ref: ${{ github.ref }}"
        echo "SHA: $(git rev-parse HEAD)"

        # Verify all production modules can be imported on the merge commit.
        # This catches cases where PR head compiles fine in isolation but
        # conflicts with changes on main (e.g., removed imports, renamed modules).
        python -c "
        import importlib, sys, pathlib
        errors = []
        # Core production modules that must import cleanly
        for mod in [
            'mvp_site.main', 'mvp_site.world_logic', 'mvp_site.llm_service',
            'mvp_site.game_state', 'mvp_site.firestore_service',
            'mvp_site.structured_fields_utils', 'mvp_site.streaming_orchestrator',
        ]:
            try:
                importlib.import_module(mod)
                print(f'  âœ“ {mod}')
            except Exception as e:
                errors.append((mod, e))
                print(f'  âœ— {mod}: {e}')
        if errors:
            print(f'\n{len(errors)} import(s) failed on merge commit!')
            sys.exit(1)
        print('\nAll core imports validated on merge commit.')
        "

    - name: Run merge-commit smoke tests
      run: |
        # Run a focused subset of fast tests on the merge commit.
        # These are chosen to exercise cross-module integration points.
        python -m pytest \
          mvp_site/tests/test_main_state_helper.py \
          mvp_site/tests/test_game_state.py \
          -x -q --timeout=60 --no-header \
          2>&1 || {
            echo "::error::Merge-commit smoke tests failed â€” PR may have integration issues with base branch"
            exit 1
          }

  # Directory-based test execution
  test:
    name: Directory tests (${{ matrix.test-group }})
    needs: detect-changes
    if: needs.detect-changes.outputs.has-changes == 'true'
    runs-on: ubuntu-latest
    # Core group runs with coverage enabled and sequential execution, which can exceed 15m.
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.detect-changes.outputs.matrix) }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4.3.1  # v4.1.1
      with:
        # For PRs: checkout the actual head commit, not the synthetic merge commit.
        # testmon fingerprints files using the working tree state; the merge commit
        # creates a different checkout that defeats testmon's change detection.
        ref: ${{ github.event.pull_request.head.sha || github.sha }}
        submodules: recursive
        fetch-depth: 0  # Full history needed for git diff in JS test step

    - name: Set up Python 3.11
      uses: actions/setup-python@0a5c61591373683505ea898e09a3ea4f39ef2b9c  # v5.0.0
      with:
        python-version: '3.11'

    - name: Set up Node.js 20
      if: matrix.test-group == 'core-mvp-1'
      uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8  # v4.0.2
      with:
        node-version: '20'

    - name: Cache pip dependencies
      uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
      with:
        path: |
          ~/.cache/pip
        key: ${{ runner.os }}-python-3.11-${{ hashFiles('**/requirements.txt') }}-${{ matrix.test-group }}
        restore-keys: |
          ${{ runner.os }}-python-3.11-${{ hashFiles('**/requirements.txt') }}-
          ${{ runner.os }}-python-3.11-

    - name: Cache virtualenv
      uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
      with:
        path: venv
        key: ${{ runner.os }}-venv-3.11-${{ matrix.test-group }}-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}

    - name: Cache testmon data
      uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
      with:
        path: |
          .testmondata
          .testmondata-shm
          .testmondata-wal
          mvp_site/.testmondata
          mvp_site/.testmondata-shm
          mvp_site/.testmondata-wal
        # Rolling key: GitHub Actions cache is write-once per key, so a static
        # branch-based key can never be updated after first creation.
        # Using run_number makes each run save fresh testmon data under a new
        # key. restore-keys prefix match loads the most recent prior run's
        # baseline from the same branch.
        key: testmon-${{ matrix.test-group }}-${{ github.base_ref || github.ref_name }}-${{ github.run_number }}
        restore-keys: |
          testmon-${{ matrix.test-group }}-${{ github.base_ref || github.ref_name }}-

    - name: Debug testmon state
      if: always()
      run: |
        echo "=== Git checkout info ==="
        echo "HEAD: $(git rev-parse HEAD)"
        echo "github.sha: ${{ github.sha }}"
        echo "PR head SHA: ${{ github.event.pull_request.head.sha }}"
        echo "Event: ${{ github.event_name }}"
        echo ""
        echo "=== Testmon data files ==="
        ls -la .testmondata mvp_site/.testmondata 2>/dev/null || echo "No .testmondata files found"
        echo ""
        if [ -f mvp_site/.testmondata ] || [ -f .testmondata ]; then
          DB_FILE="mvp_site/.testmondata"
          [ ! -f "$DB_FILE" ] && DB_FILE=".testmondata"
          echo "=== Testmon DB tables ==="
          sqlite3 "$DB_FILE" ".tables" 2>/dev/null || echo "Cannot read DB"
          echo ""
          echo "=== Testmon file fingerprints (first 20) ==="
          sqlite3 "$DB_FILE" "SELECT * FROM file LIMIT 20;" 2>/dev/null || echo "No file table"
          echo ""
          echo "=== Testmon node count ==="
          sqlite3 "$DB_FILE" "SELECT COUNT(*) FROM node;" 2>/dev/null || echo "No node table"
          echo ""
          echo "=== Testmon environment ==="
          sqlite3 "$DB_FILE" "SELECT * FROM metadata;" 2>/dev/null || echo "No metadata table"
        fi

    - name: Install dependencies
      run: |
        # Reuse cached venv when available; otherwise create it
        if [ -d venv ] && [ -x venv/bin/python ]; then
          echo "Reusing cached venv"
        else
          echo "Creating new venv"
          python -m venv venv
        fi
        source venv/bin/activate

        if python -c "import pydantic, pytest, xdist, pytest_cov, testmon" 2>/dev/null; then
          echo "Cached venv already has core dependencies"
        else
          # Upgrade pip and install resolver tools with timeout
          timeout 300 python -m pip install --upgrade pip setuptools wheel

          # Install core application requirements
          timeout 600 pip install -r mvp_site/requirements.txt

          # Install pytest tooling for batched worker execution
          timeout 300 pip install pytest pytest-xdist pytest-testmon pytest-cov
        fi

        # Install additional requirements for specific directories if needed
        # NOTE: Keep the group names in sync with GROUP_CONFIG in scripts/ci-detect-changes.sh (source of truth).
        case "${{ matrix.test-group }}" in
          "core-mvp-1"|"core-mvp-2"|"core-mvp-3"|"core-tests"|"claude"|"orchestration"|"automation"|"scripts"|"mcp")
            # These directories may have additional requirements
            find . -name "requirements.txt" -not -path "./venv/*" -not -path "./mvp_site/requirements.txt" | while read req_file; do
              echo "Installing $req_file (with 300s timeout)"
              timeout 300 pip install -r "$req_file" || echo "Warning: Failed to install $req_file"
            done
            # Install utility packages if testing core group (tests/ directory needs these)
            if [[ "${{ matrix.test-group }}" == core-* ]]; then
              if [ -f "moltbook_poster/pyproject.toml" ]; then
                echo "Installing moltbook_poster package for tests/ directory"
                timeout 300 pip install -e "./moltbook_poster" || echo "Warning: Failed to install moltbook_poster"
              fi
              if [ -f "orchestration/pyproject.toml" ]; then
                echo "Installing orchestration package for tests/ directory"
                timeout 300 pip install -e "./orchestration" || echo "Warning: Failed to install orchestration"
              fi
            fi
            # Install automation package if testing automation
            if [ "${{ matrix.test-group }}" = "automation" ] && [ -f "automation/pyproject.toml" ]; then
              echo "Installing automation package in editable mode with dev dependencies"
              timeout 300 pip install -e "./automation[dev]"
            fi
            ;;
        esac

        # Install coverage for core group (data collected during test execution)
        if [[ "${{ matrix.test-group }}" == core-* ]]; then
          pip install coverage
        fi

        # Verify key dependencies
        timeout 60 python -c "import pydantic; print(f'âœ… Pydantic {pydantic.__version__} available')"
        chmod +x vpython
        timeout 60 ./vpython --version

    - name: Run frontend_v1 JS unit tests
      if: matrix.test-group == 'core-mvp-1'
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          CHANGED_FILES=$(git diff --name-only "${{ github.event.pull_request.base.sha }}" "${{ github.event.pull_request.head.sha }}")
        else
          CHANGED_FILES=$(git diff --name-only "HEAD~1..HEAD")
        fi

        if echo "$CHANGED_FILES" | grep -q '^mvp_site/frontend_v1/'; then
          node --test mvp_site/frontend_v1/tests/settings_listeners.test.js
        else
          echo "No frontend_v1 changes detected; skipping JS unit test."
        fi

    - name: Create coverage config
      if: startsWith(matrix.test-group, 'core-')
      run: |
        # Must exist BEFORE test execution so coverage stores relative paths.
        # Without this, the coverage-report job can't resolve source files.
        cat > .coveragerc << 'COVEOF'
        [run]
        relative_files = true
        source = mvp_site
        omit =
            */tests/*
            */testing_framework/*
            */venv/*
            */__pycache__/*

        [report]
        exclude_lines =
            pragma: no cover
            def __repr__
            raise NotImplementedError
        COVEOF

    - name: Run directory-based tests - ${{ matrix.test-group }}
      # Keep per-step timeout aligned with the expanded suite timeout budget.
      timeout-minutes: 25
      run: |
        source venv/bin/activate

        # Set CI environment variables
        export TESTING=true
        export TESTING_AUTH_BYPASS=true
        export MOCK_SERVICES_MODE=true
        export FAST_TESTS=1
        # Defense-in-depth: Set fake API keys so real services can't be hit
        # even if mock mode is somehow bypassed
        # NOTE: Keys must start with "test-" or "dummy-" to trigger Gemini stub
        # (see gemini_provider._use_test_stub_client)
        export GEMINI_API_KEY="test-ci-key-only"
        export CEREBRAS_API_KEY="test-ci-key-only"
        export OPENROUTER_API_KEY="test-ci-key-only"
        export TEST_GEMINI_API_KEY="test-ci-key-only"
        export GOOGLE_APPLICATION_CREDENTIALS="/dev/null"
        # Disable semantic classifier to prevent model download stalls in CI
        export ENABLE_SEMANTIC_ROUTING=false
        export ENABLE_BROWSER_TESTS=0
        export ENABLE_BUILD_TESTS=0
        export ENABLE_NETWORK_TESTS=0
        export GITHUB_ACTIONS=true
        export PYTHONPATH="${PYTHONPATH}:${PWD}:${PWD}/mvp_site:${PWD}/automation"
        export TEST_SHARD_INDEX="${{ matrix.shard-index }}"
        export TEST_SHARD_TOTAL="${{ matrix.shard-total }}"

        echo "ðŸŽ¯ Running tests for directory group: ${{ matrix.test-group }}"
        echo "ðŸ“‚ Test directories: ${{ matrix.test-dirs }}"
        echo "ðŸ§© Shard: ${TEST_SHARD_INDEX}/${TEST_SHARD_TOTAL}"

        if [ "${{ matrix.test-group }}" = "automation" ]; then
          echo "ðŸ§ª Running automation shell regression tests"
          bash automation/tests/test_install_cron_entries.sh
          bash automation/tests/test_crontab_restore_safety.sh
        fi

        # Run tests only for the specified directory/directories
        chmod +x run_tests.sh
        # CI_TEST_LIMIT removed to run all discovered tests (prevents skipping critical tests)
        # Historical note: CI_TEST_LIMIT=50 previously truncated test lists, risking test coverage gaps

        # Use directory-based test discovery with validation
        IFS=',' read -ra SELECTED_DIRS <<< "${{ matrix.test-dirs }}"
        TOTAL_TESTS=0
        for DIR in "${SELECTED_DIRS[@]}"; do
          DIR=$(echo "$DIR" | xargs)
          [ -z "$DIR" ] && continue
          COUNT=$(find "$DIR" -name "test_*.py" -type f 2>/dev/null | wc -l)
          echo "ðŸ“Š Found $COUNT test files in $DIR"
          TOTAL_TESTS=$((TOTAL_TESTS + COUNT))
        done

        if [ "$TOTAL_TESTS" -eq 0 ]; then
          echo "âš ï¸ No tests found in specified directories, falling back to full test suite"
          ./run_tests.sh --ci --parallel --exclude-integration --exclude-mcp
        elif [[ "${{ matrix.test-group }}" == core-mvp-* ]] && [ "${{ github.event_name }}" = "pull_request" ]; then
          # Coverage shards on PRs: testmon skips unchanged tests,
          # pytest-cov measures coverage on what ran. If testmon deselects a
          # test, its coverage is unchanged â€” the total stays accurate.
          export TEST_SUITE_TIMEOUT=1080  # 18 min (vs default 15 min)
          export TEST_MAX_WORKERS=2
          ./run_tests.sh --test-dirs="${{ matrix.test-dirs }}" --coverage --exclude-integration --exclude-mcp --testmon
        elif [ "${{ github.event_name }}" = "push" ]; then
          # Push-to-main: use testmon to build/update the .testmondata baseline.
          # On subsequent pushes, testmon skips unchanged test files.
          export TEST_MAX_WORKERS=2
          ./run_tests.sh --test-dirs="${{ matrix.test-dirs }}" --parallel --exclude-integration --exclude-mcp --testmon
        else
          # Non-coverage shards on PRs (e.g. core-tests): use testmon for test reduction.
          # Ghost tests fixed â€” all tests now pass under pytest, enabling testmon.
          export TEST_MAX_WORKERS=2
          ./run_tests.sh --test-dirs="${{ matrix.test-dirs }}" --parallel --exclude-integration --exclude-mcp --testmon
        fi

    - name: Checkpoint testmon WAL
      if: always()
      run: |
        # SQLite WAL mode keeps data in -wal files. Checkpoint flushes
        # all data to the main .testmondata so the cache step captures it.
        for db in .testmondata mvp_site/.testmondata; do
          if [ -f "$db" ]; then
            echo "Checkpointing $db"
            sqlite3 "$db" "PRAGMA wal_checkpoint(TRUNCATE);" 2>/dev/null || true
            ls -la "${db}"* 2>/dev/null
          fi
        done

    - name: Upload test results
      uses: actions/upload-artifact@5d5d22a31266ced268874388b861e4b58bb5c2f3  # v4.3.1
      if: always()
      with:
        name: test-results-${{ matrix.test-group }}
        path: |
          mvp_site/test-results/
          mvp_site/*.log
        retention-days: 30
        if-no-files-found: warn

    - name: Prepare coverage data
      if: always() && startsWith(matrix.test-group, 'core-mvp-') && github.event_name == 'pull_request'
      run: |
        # Copy coverage data from run_tests.sh output location to workspace
        if [ -f /tmp/worldarchitectai/coverage/.coverage ]; then
          cp /tmp/worldarchitectai/coverage/.coverage ".coverage.${{ matrix.test-group }}"
          echo "âœ… Coverage data copied to workspace"
        elif compgen -G "/tmp/worldarchitectai/coverage/.coverage.*" > /dev/null; then
          source venv/bin/activate
          python -m coverage combine /tmp/worldarchitectai/coverage || true
          if [ -f .coverage ]; then
            cp .coverage ".coverage.${{ matrix.test-group }}"
            echo "âœ… Coverage data combined and copied from shard files"
          elif [ -f /tmp/worldarchitectai/coverage/.coverage ]; then
            cp /tmp/worldarchitectai/coverage/.coverage ".coverage.${{ matrix.test-group }}"
            echo "âœ… Coverage data combined and copied from coverage dir"
          else
            echo "::warning::Coverage combine did not produce .coverage output"
          fi
        elif [ -f .coverage ]; then
          cp .coverage ".coverage.${{ matrix.test-group }}"
          echo "âœ… Coverage data copied from workspace"
        else
          echo "::warning::No coverage data found"
        fi

    - name: Upload coverage data
      if: always() && startsWith(matrix.test-group, 'core-mvp-') && github.event_name == 'pull_request'
      uses: actions/upload-artifact@5d5d22a31266ced268874388b861e4b58bb5c2f3  # v4.3.1
      with:
        name: coverage-data-${{ matrix.test-group }}
        path: .coverage.${{ matrix.test-group }}
        if-no-files-found: warn

  # Coverage reporting - processes data from core test group, no duplicate test execution
  coverage-report:
    name: PR Coverage Report
    needs: [test]
    if: always() && github.event_name == 'pull_request' && needs.test.result != 'cancelled'
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    timeout-minutes: 5

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4.3.1  # v4.1.1
        with:
          fetch-depth: 0  # Full history needed for git diff (delta coverage)

      - name: Set up Python 3.11
        uses: actions/setup-python@0a5c61591373683505ea898e09a3ea4f39ef2b9c  # v5.0.0
        with:
          python-version: '3.11'

      - name: Download coverage data
        uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16  # v4.1.8
        with:
          pattern: coverage-data-*
          merge-multiple: true
          path: coverage-data
        continue-on-error: true

      - name: Identify changed Python files
        id: changed_files
        run: |
          # Delta coverage: enforce threshold only on files the PR actually changed.
          # This is compatible with testmon's selective execution â€” even when
          # testmon deselects unchanged tests, coverage data for the changed
          # files comes from the tests that DID run.
          BASE_SHA=${{ github.event.pull_request.base.sha }}
          HEAD_SHA=${{ github.event.pull_request.head.sha }}
          echo "Base: $BASE_SHA"
          echo "Head: $HEAD_SHA"

          CHANGED_PY=$(git diff --name-only --diff-filter=ACMR "$BASE_SHA"..."$HEAD_SHA" -- '*.py' \
            | grep -v '__pycache__' \
            | grep -v '/tests/' \
            | grep -v '/testing_framework/' \
            | grep -v '/venv/' \
            || true)

          if [ -z "$CHANGED_PY" ]; then
            echo "has_changed_py=false" >> $GITHUB_OUTPUT
            echo "::notice::No production Python files changed â€” coverage threshold not applicable"
          else
            echo "has_changed_py=true" >> $GITHUB_OUTPUT
            FILE_COUNT=$(echo "$CHANGED_PY" | wc -l | tr -d ' ')
            echo "changed_count=$FILE_COUNT" >> $GITHUB_OUTPUT
            echo "::notice::$FILE_COUNT production Python file(s) changed â€” delta coverage will be checked"
            echo ""
            echo "Changed production files:"
            echo "$CHANGED_PY" | sed 's/^/  /'

            # Build comma-separated include pattern for coverage CLI
            INCLUDE_PATTERN=$(echo "$CHANGED_PY" | paste -sd ',' -)
            echo "include_pattern=$INCLUDE_PATTERN" >> $GITHUB_OUTPUT
          fi

      - name: Generate coverage report
        id: coverage_check
        run: |
          pip install coverage

          # Create .coveragerc matching the coverage.yml baseline config
          cat > .coveragerc << 'COVEOF'
          [run]
          relative_files = true
          source = mvp_site
          omit =
              */tests/*
              */testing_framework/*
              */venv/*
              */__pycache__/*

          [report]
          exclude_lines =
              pragma: no cover
              def __repr__
              raise NotImplementedError
          COVEOF

          if compgen -G "coverage-data/.coverage*" > /dev/null; then
            echo "has_coverage=true" >> $GITHUB_OUTPUT
            coverage combine coverage-data || echo "Coverage combine skipped"
            coverage report --format=markdown > coverage_report.md || echo "Coverage report generation failed"
            coverage xml -o coverage.xml || echo "Coverage XML generation failed"
            coverage json -o coverage.json || echo "Coverage JSON generation failed"
          else
            echo "has_coverage=false" >> $GITHUB_OUTPUT
            echo "::warning::No coverage data available from test run"
          fi

      - name: Coverage comment (PR)
        if: steps.coverage_check.outputs.has_coverage == 'true'
        uses: py-cov-action/python-coverage-comment-action@fb02115d6115e7b3325dc3295fe1dcfb1919248a  # v3.32
        with:
          GITHUB_TOKEN: ${{ github.token }}
          MINIMUM_GREEN: 80
          MINIMUM_ORANGE: 60
          ANNOTATE_MISSING_LINES: true
          MAX_FILES_IN_COMMENT: 50

      - name: Check delta coverage threshold (per-file)
        if: steps.coverage_check.outputs.has_coverage == 'true' && steps.changed_files.outputs.has_changed_py == 'true'
        run: |
          # Per-file delta coverage: evaluate each changed file independently.
          # This prevents well-tested files from masking under-tested ones
          # in multi-file PRs (and vice versa).

          # --- Exception list: known low-coverage legacy files ---
          # These get a reduced threshold until test coverage improves.
          # Standard files: 60% minimum, 80% recommended
          # Exception files: 30% minimum (still enforces SOME coverage)
          LOW_COVERAGE_FILES="mvp_site/main.py mvp_site/start_flask.py"
          STANDARD_MIN=60
          STANDARD_REC=80
          REDUCED_MIN=30

          echo "=== Per-file delta coverage check ==="
          echo "Standard threshold: ${STANDARD_MIN}% min / ${STANDARD_REC}% recommended"
          echo "Reduced threshold (legacy files): ${REDUCED_MIN}% min"
          echo "Legacy exception list: $LOW_COVERAGE_FILES"
          echo ""

          FAILED=0
          WARNED=0
          PASSED=0
          SKIPPED=0
          SUMMARY=""

          IFS=',' read -ra FILES <<< "${{ steps.changed_files.outputs.include_pattern }}"
          for FILE in "${FILES[@]}"; do
            # Get coverage for this single file
            FILE_REPORT=$(coverage report --include="$FILE" 2>/dev/null || true)

            if [ -z "$FILE_REPORT" ]; then
              SKIPPED=$((SKIPPED + 1))
              SUMMARY="${SUMMARY}âšª ${FILE}: no coverage data (skipped)\n"
              continue
            fi

            # Parse coverage percentage
            COV=$(echo "$FILE_REPORT" | grep -v "^Name\|^-" | grep "$FILE" | awk '{print $NF}' | tr -d '%')
            if ! [[ "$COV" =~ ^[0-9]+$ ]]; then
              COV=$(echo "$FILE_REPORT" | tail -1 | awk '{print $NF}' | tr -d '%')
            fi

            if ! [[ "$COV" =~ ^[0-9]+$ ]]; then
              SKIPPED=$((SKIPPED + 1))
              SUMMARY="${SUMMARY}âšª ${FILE}: coverage parse failed (skipped)\n"
              continue
            fi

            # Determine threshold for this file
            IS_EXCEPTION=false
            for EX in $LOW_COVERAGE_FILES; do
              if [ "$FILE" = "$EX" ]; then
                IS_EXCEPTION=true
                break
              fi
            done

            if [ "$IS_EXCEPTION" = true ]; then
              MIN_THRESH=$REDUCED_MIN
              THRESH_LABEL="reduced"
            else
              MIN_THRESH=$STANDARD_MIN
              THRESH_LABEL="standard"
            fi

            # Evaluate
            if [ "$COV" -lt "$MIN_THRESH" ]; then
              FAILED=$((FAILED + 1))
              SUMMARY="${SUMMARY}âŒ ${FILE}: ${COV}% (${THRESH_LABEL} min: ${MIN_THRESH}%)\n"
              echo "::error::${FILE} coverage ${COV}% is below ${THRESH_LABEL} minimum of ${MIN_THRESH}%"
            elif [ "$IS_EXCEPTION" = false ] && [ "$COV" -lt "$STANDARD_REC" ]; then
              WARNED=$((WARNED + 1))
              SUMMARY="${SUMMARY}âš ï¸  ${FILE}: ${COV}% (below recommended ${STANDARD_REC}%)\n"
              echo "::warning::${FILE} coverage ${COV}% is below recommended ${STANDARD_REC}%"
            else
              PASSED=$((PASSED + 1))
              if [ "$IS_EXCEPTION" = true ]; then
                SUMMARY="${SUMMARY}âœ… ${FILE}: ${COV}% (legacy file, reduced threshold)\n"
              else
                SUMMARY="${SUMMARY}âœ… ${FILE}: ${COV}%\n"
              fi
            fi
          done

          echo ""
          echo "=== Per-file results ==="
          echo -e "$SUMMARY"
          echo "Totals: $PASSED passed, $WARNED warned, $FAILED failed, $SKIPPED skipped"

          # Show overall project coverage for context
          TOTAL_COV=$(coverage report 2>/dev/null | grep "^TOTAL" | awk '{print $NF}' | tr -d '%' || echo "?")
          echo "Overall project coverage: ${TOTAL_COV}%"

          if [ "$FAILED" -gt 0 ]; then
            echo ""
            echo "::error::$FAILED file(s) failed their coverage threshold"
            exit 1
          fi

      - name: Post coverage summary
        if: always()
        run: |
          echo "## Coverage Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f coverage_report.md ]; then
            cat coverage_report.md >> $GITHUB_STEP_SUMMARY
          else
            echo "No coverage data available for this PR." >> $GITHUB_STEP_SUMMARY
          fi
