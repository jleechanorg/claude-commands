---
description: 
globs: 
alwaysApply: true
---
# Core Operating Protocol for AI Collaboration

**CRITICAL RULE: NO FALSE GREEN CHECKMARKS**
NEVER use ‚úÖ green checkmarks unless the feature/test/functionality works 100% completely. Green checkmarks indicate FULL COMPLETION AND SUCCESS. If something is partially done, timed out, has errors, or is "ready but not run", use ‚ùå ‚ö†Ô∏è üîÑ or plain text. Green checkmarks create false impressions of completion when things are actually broken or incomplete.

**CRITICAL RULE: NO POSITIVITY - BE EXTREMELY SELF-CRITICAL**
I must be extremely hard on myself and self-critical. No celebration unless things work 100% - not 99%, not 99.9%. No positive language about partial progress, "infrastructure ready", or "almost working". If it doesn't work completely, it's a failure. Focus on what's broken, what failed, what's missing. No encouraging language or optimistic framing.

**CRITICAL RULE: NEVER SIMULATE ANYTHING**
Never simulate anything. Always ask if you are having trouble. A fake answer is 1000x worse than getting stuck. We need accuracy.

**PRIMARY RULE SOURCE: This file (.cursor/rules/rules.mdc) is the single source of truth for all operating protocols, development guidelines, and project-specific rules. CLAUDE.md should only contain Claude Code specific behavior.**

**Meta-Rule: Before beginning any task, you must check for the existence of a .cursor/rules/rules.mdc file in the workspace root. If this file exists, you must read it and treat its contents as your primary operating protocol, superseding any other general instructions.**

** anchoring rule:** The `.cursor` directory at the absolute top level of the workspace is the single source of truth for all protocol and lessons files. Any instruction to modify rules, lessons, or project documentation refers *exclusively* to the files within this top-level `.cursor` directory (e.g., `.cursor/rules/rules.mdc`). I will not create, read, or write rule files in any other location.

**PLANNING AND DEVELOPMENT APPROACHES: For all planning methodologies including scratchpad protocol, milestone planning, multi-perspective approach, and plan review protocol, see `.cursor/rules/planning_protocols.md`. This unified document provides decision trees and guidance for choosing the right approach based on task complexity.**

**MODE SELECTION PROTOCOL: When starting any task or project, if not already specified by the user, I MUST ask which mode to use: single-agent approach, multi-perspective system (SUPERVISOR/WORKER/REVIEWER), or another planning approach. When using multi-perspective system, I MUST always indicate which mode ([SUPERVISOR MODE], [WORKER MODE], or [REVIEWER MODE]) I am currently operating in.**

This document outlines the operating protocol for our collaboration. It merges general best practices with specific lessons learned from our work on this project.

## Project Overview

WorldArchitect.AI is an AI-powered tabletop RPG platform that serves as a digital Game Master for D&D 5e experiences. 

For complete project details including technology stack, architecture, development commands, and constraints, see `.cursor/rules/project_overview.md`.

### Additional Documentation Resources
- **Documentation Map**: For a visual hierarchy and navigation guide to all documentation, see `.cursor/rules/documentation_map.md`
- **Quick Reference**: For common commands and quick lookups, see `.cursor/rules/quick_reference.md`
- **Progress Tracking Template**: For milestone progress tracking, see `roadmap/templates/progress_tracking_template.md`
- **Directory Structure**: For comprehensive project directory layout and file locations, see `/directory_structure.md`

## Directory and Path Conventions

### Important Path References
- **roadmap/**: ALWAYS refers to `/roadmap/` from the project root, never a subdirectory
- **mvp_site/**: The main application directory containing all production code
- **prototype/**: Experimental code and prototypes (separate from production)
- **.cursor/**: Project rules and documentation (at workspace root only)

### When Writing Documentation
- Always use paths from project root (e.g., `/roadmap/file.md`, `/mvp_site/prompts/`)
- Never use relative paths that could be ambiguous
- When user says "roadmap/", they mean the roadmap directory at project root

## I. Core Principles & Interaction

1.  **Clarify and Understand the Goal:**
    *   Before initiating any work, I will ensure I have a complete and unambiguous understanding of your task. If anything is unclear, I will ask for clarification immediately.

2.  **Your Instructions are Law:**
    *   Your explicit instructions regarding code, component names, and file contents are the absolute source of truth.

3.  **Deletion is Prohibited Without Explicit Confirmation:**
    *   I am strictly prohibited from deleting any asset‚Äîbe it a file, directory, or lines of code‚Äîwithout first proposing the exact deletion and receiving explicit, affirmative confirmation from you. My default operation must always be to preserve and add, never to remove, without your consent.

4.  **Leave Working Code Alone & Adhere to Protocol:**
    *   I will not modify functional code to satisfy linters or for any other non-essential reason without your explicit permission.
    *   I will review these rules before every response to ensure I am in full compliance.

5.  **Focus on the Primary Goal:**
    *   I will prioritize the user's most recent, explicit goal above all else. I must not get sidetracked by secondary issues like linter warnings or unrelated code cleanup unless explicitly instructed to do so.

6.  **Propose and Confirm:**
    *   My primary mode of operation is to propose a solution for your confirmation before implementing it, especially for complex changes.

7.  **Acknowledge Key Takeaways:**
    *   I will summarize important points after major steps or debugging sessions to ensure we are aligned.

8.  **Externalize Knowledge for Transparency:**
    *   As a core directive, I recognize that you have no access to my internal memories. Therefore, all important rules, project-specific knowledge, and lessons learned **must** be externalized into the appropriate files within the `.cursor/` directory (`rules/rules.mdc`, `rules/lessons.mdc`, `project.md`).

9.  **Rule Synchronization Protocol:**
    *   Any instruction to "add a rule," "change the rules," or a similar directive refers specifically to this file: `.cursor/rules/rules.mdc`.
    *   When such a directive is given, I **must** update both my internal memories and this file in parallel.
    *   If the directive is ambiguous in any way, I must ask for clarification before proceeding.

10. **General Principles Over Specific Details:**
    *   Both `.cursor/rules/rules.mdc` and `CLAUDE.md` must contain **general principles and protocols** only
    *   **Specific technical failures, code patterns, and detailed incident analysis** belong in `.cursor/rules/lessons.mdc`
    *   **Rules files** establish timeless operational principles; **lessons files** capture specific technical learning
    *   When adding content, ask: "Is this a general principle or a specific technical detail?" and place accordingly

11. **Verify Edits Before Concluding:**
    *   After every file edit, I must not assume the change was applied correctly. I will use my tools (`git diff` or `read_file`) to verify that the change exactly matches my intention before proceeding. This is especially critical for deletions, reverts, and changes to rule files.

12. **Ignore Firestore Linter Errors:**
    *   I will disregard any linter errors originating from Firebase/Firestore code and assume the code is functional unless you instruct me otherwise.

13. **Edits Must Be Additive or Surgical:**
    *   When modifying a file, my changes must be strictly additive (adding new code) or surgical (modifying specific lines). I must never delete existing, unrelated code, especially tests. I will verify this by carefully inspecting the diff after every `edit_file` operation.

14. **Propose Failing Tests:**
    *   When debugging a bug that is not covered by the existing test suite, I must first propose a new unit test that reliably reproduces the failure (a "red" test). I will explain why the test is expected to fail and will not add it to the code until you explicitly approve it.

15. **Red-Green Testing Methodology:**
    *   When implementing new features or functionality, I must follow the red-green testing approach: First, write a unit test that fails without the new implementation ("red" state), then implement the feature to make the test pass ("green" state). This ensures the test actually validates the new functionality and catches regressions.
    *   **For UI Features**: Create functional tests that reproduce specific user issues, run tests to confirm they FAIL (red state), then fix implementation to make tests PASS (green state).
    *   **Critical Mindset**: "I haven't seen it work with my own eyes yet" - Never claim features work based on code alone. Must validate actual user experience.

16. **Test Truth Verification Protocol:**
    *   **MANDATORY**: Before making any architectural decision based on test results, verify the test is testing what it claims:
        ```python
        # Required at start of every comparison test
        print(f"TEST VERIFICATION: Testing {approach_name}")
        print(f"  Module: {module.__name__}")
        print(f"  Has expected base: {'BaseModel' in str(Model.__bases__)}")
        print(f"  Dependencies present: {verify_dependencies()}")
        ```
    *   **Negative Test Requirement**: For any validation approach, MUST test cases that should be rejected
    *   **Architecture Decision Tests (ADTs)**: Create separate tests that verify architectural decisions remain valid
    *   **No Duplicate Implementations**: Actively check for and prevent multiple implementations of the same concept

17. **Security: DO NOT SUBMIT Rule:**
    *   If I see "DO NOT SUBMIT" anywhere in code, comments, or environment variables, I must never allow this to be committed or merged into any branch. This is a critical security marker indicating sensitive data (API keys, credentials, etc.) that must be removed before any PR or commit.

18. **Unit Test Content Independence:**
    *   **CRITICAL**: Unit tests must test behavior and logic, NOT exact content strings
    *   **Bad Practice**: `self.assertIn("CRITICAL NAMING RESTRICTIONS", content)` - brittle, breaks when wording changes
    *   **Good Practice**: `self.assertTrue(loader.has_banned_names_section())` - tests behavior, not implementation
    *   **Content Testing Guidelines**:
        - Test that required sections/data exist, not their exact wording
        - Test that functions return expected types/structures, not exact strings
        - Test that validation logic works, not specific error messages
        - Use helper methods that abstract content checks (e.g., `contains_section(name)`)
    *   **Exception**: Only test exact content when it's a critical API contract that must never change
    *   **Refactoring Safety**: Tests should survive content updates, rewording, and formatting changes

## II. Development, Coding & Architecture

1.  **Preservation Over Efficiency:**
    *   My most critical coding directive is to treat existing code as a fixed template. I will make surgical edits and will not delete or refactor working code without your explicit permission.

2.  **String Constant Management:**
    *   To improve maintainability and prevent errors from typos, string literals must be managed as constants.
    *   **Rule:** If a string literal is used more than once within a single file, it must be defined as a module-level constant (e.g., `MY_CONSTANT = "my_string"`).
    *   **Rule:** If a string literal is used across multiple files (e.g., as a dictionary key in a data structure), it must be defined in the global `constants.py` file and imported where needed.

3.  **DRY (Don't Repeat Yourself):**
    *   Code used in multiple places should be refactored into a helper function to improve maintainability.

4.  **Robust Data Handling:**
    *   Application code must be resilient to data variations, handle API limits gracefully, and manage different data schemas within the application logic.

5.  **Case-Insensitive Field Detection:**
    *   When searching for fields that may have inconsistent naming (e.g., AI-generated data), implement case-insensitive searches and handle multiple possible field names (e.g., `hp_current`, `HP_current`, `hp`).

6.  **Defensive Data Structure Handling:**
    *   Before calling methods on data structures, always verify the data type. Collections from external sources may contain mixed types (strings, dictionaries, etc.) that require different handling approaches.

7.  **Professional-Grade Development Practices:**
    *   I will follow standard best practices, including: using the `logging` module in Python, using docstrings, and ensuring all DOM-manipulating JavaScript is properly loaded.

8.  **Verify, Don't Assume:**
    *   I will use my tools to check the current state of the codebase (e.g., API method signatures, library versions) before making assumptions.

9.  **Use the Correct Gemini SDK:**
    *   This project uses the modern `google.genai` Python SDK (NOT `google.generativeai`). All Gemini API calls **must** use:
        - Import: `from google import genai` and `from google.genai import types`
        - Initialization: `client = genai.Client(api_key=api_key)`
        - API calls: `client.models.generate_content(model=MODEL_NAME, contents=prompt)`
    *   NEVER use the legacy `google.generativeai` package or `genai.GenerativeModel()` pattern.

10. **Do Not Change the AI Model:**
    *   **AI Models**: Current models are `DEFAULT_MODEL = 'gemini-2.5-flash'`, `LARGE_CONTEXT_MODEL = 'gemini-2.5-pro'`, `TEST_MODEL = 'gemini-1.5-flash'` - never change without explicit user authorization

11. **Snippet-Based Code Modification:**
    *   By default, I will provide targeted code snippets with precise instructions on where to integrate them, rather than replacing entire files.

12. **No Unsolicited Refactoring:**
    *   I will not perform any cleanup, refactoring, or other changes that are not directly part of the assigned task. I may suggest these changes, but I must await your explicit approval before implementing them.

13. **Always Use Temporary Files for Testing:**
    *   When creating test files or any files that might overwrite user data, I must always use temporary directories (e.g., `tempfile.mkdtemp()`) to avoid overwriting real project files. This prevents data loss and ensures test isolation.

14. **Defensive Programming for Dynamic Data:**
    *   When processing data from external sources (databases, APIs, AI responses), I must always validate data types before attempting operations. Use `isinstance(data, expected_type)` checks before calling type-specific methods like `.get()`, `.append()`, etc.
    *   **Pattern:** For dictionary iteration, always check `if isinstance(item, dict):` before using dictionary methods.

15. **Integration Test Design Principles:**
    *   Integration tests should work with natural application state rather than pre-setting conflicting test data.
    *   Use flexible assertions that verify behavior rather than exact values when testing AI-generated content.
    *   Optimize test speed by using faster AI models (`gemini-1.5-flash`) and shared test fixtures when possible.
    *   Always use proper temporary directories to avoid overwriting real application files during testing.

16. **Global Event Listener Safety Protocol:**
    *   **PROHIBITION**: Never use `document.addEventListener('click', ...)` or other global DOM event listeners without explicit user approval
    *   **REQUIREMENT**: Always use targeted event binding to specific elements instead of document-wide listeners
    *   **ANALYSIS**: Any global listener must include detailed justification of why targeted binding is insufficient
    *   **PATTERN**: Use `element.addEventListener()` or delegate only within specific containers (`container.addEventListener`)
    *   **SELECTOR SAFETY**: Avoid broad CSS selectors that might match unintended elements (e.g., `[data-theme]` matching document root)
    *   **EXCEPTION**: Only use global listeners for genuinely global concerns (keyboard shortcuts, window resize, system-level events)

17. **Cross-Cutting Concern Integration Analysis:**
    *   **MANDATORY**: Before implementing any feature that modifies DOM, CSS, or JavaScript, explicitly categorize the change:
        - **Surface Feature**: Only affects specific UI components (safe)
        - **Cross-Cutting Concern**: Touches multiple systems (requires integration analysis)
        - **Infrastructure Change**: Modifies core application behavior (very dangerous)
    *   **FOR Cross-Cutting Concerns**: Mandatory integration analysis of ALL existing systems before implementation
    *   **ANALYSIS REQUIRED**: Event handling patterns, DOM manipulation scope, CSS cascade impact, global state changes
    *   **BLAST RADIUS**: Document all systems the change will touch before proceeding

19. **Core User Workflow Protection:**
    *   **IDENTIFICATION**: Before implementing any system modification, identify the top 3-5 core user workflows
    *   **CORE WORKFLOWS**: Authentication, Campaign Creation, Game Interaction, Data Persistence, Form Submissions
    *   **MANDATORY TESTING**: Test each core workflow after ANY system modification, regardless of perceived scope
    *   **VALIDATION**: All core workflows must function identically before and after feature addition
    *   **ROLLBACK TRIGGER**: ANY core workflow failure requires immediate feature rollback

20. **File Naming Conventions:**
    *   **No Red/Green in Filenames:** I must never use "red", "green", "red_green", or similar color-based terminology in filenames. This avoids confusion and maintains professional naming standards.
    *   Use descriptive names that indicate the file's purpose instead (e.g., `test_comprehensive.py`, `test_integration.py`, `test_edge_cases.py`).

21. **Backup and Temporary File Management:**
    *   **MANDATORY**: All backup files (*.backup, *.orig, *.old) and temporary files (temp_*, *_temp, *_tmp) MUST be placed in the `tmp/` directory within each module
    *   **Structure**: Each module/directory should have its own `tmp/` subdirectory for organizing backups and temporary files
    *   **Never Leave Scattered**: Never leave backup or temporary files scattered throughout the codebase - always consolidate into appropriate tmp/ directories
    *   **Naming Convention**: Use descriptive names that indicate the original file and purpose (e.g., `narrative_system_instruction.md.backup`, `temp_header.txt`)
    *   **Git Tracking**: Backup files in tmp/ directories may be committed for safety, but temporary working files should generally be excluded via .gitignore

22. **Method Length Guidelines:**
    *   **CRITICAL**: Keep methods under 500 lines of code for maintainability
    *   **Extract Helper Methods**: When a method approaches this limit, extract logical sections into well-named helper methods
    *   **Single Responsibility**: Each method should have a single, clear purpose
    *   **Refactoring Priority**: When fixing bugs in long methods, prioritize extracting duplicate code into helper methods first

23. **AI-Assisted Development Time Estimation:**
    *   **CRITICAL**: Always estimate development time based on AI-assisted coding speeds, not traditional human coding
    *   **Code Generation**: Assume Claude can generate complete files, components, or modules in 1-3 minutes
    *   **Real Work Identification**: The actual time is spent on testing, integration, bug fixes, and iteration - not writing code
    *   **Time Breakdown**: For any development task:
        - Code generation by AI: 1-10 minutes  
        - Review and integration: 5-15 minutes
        - Testing and iteration: 15-45 minutes
        - Documentation and commits: 5-10 minutes
    *   **Project Estimation**: A complete frontend modernization takes 6-9 hours, not 20-40 hours
    *   **Avoid Human Coding Assumptions**: Never estimate based on how long it would take to write code manually

24. **Visual Testing and UI Validation Protocol:**
    *   **MANDATORY**: Before claiming ANY UI feature works, must run comprehensive visual validation
    *   **The "Show Me" Protocol**: Visual proof is required before marking any UI task complete
    *   **Screenshot Analysis**: When user provides screenshots showing UI problems, perform systematic visual inspection before responding
    *   **Red/Green Testing for UI**: All UI features must follow red/green methodology - create failing tests first, then implement fixes
    *   **Browser Console Validation**: Use visual validation tools (e.g., `VisualValidator.run()`) for immediate UI checks
    *   **Core Principle**: "Code that looks right can be completely broken" - never assume functionality based on code existence alone
    *   **Visual Evidence Trumps Code**: User screenshots and visual evidence override all code analysis and assumptions

25. **Screenshot-Based Problem Identification:**
    *   **CRITICAL ANALYSIS PROTOCOL**: When user provides screenshots of UI issues:
        1. Systematically analyze for overlapping elements, misaligned components, broken layouts
        2. Never assume functionality works if visual evidence suggests problems
        3. Ask specific questions about what user observes rather than making assumptions
        4. Treat visual evidence as ground truth over code logic
    *   **PREVENTION**: Screenshots reveal problems that code review cannot catch
    *   **VALIDATION**: All UI fixes must be verified through visual inspection, not just code changes

26. **Code Review Analysis Completeness:**
    *   **MANDATORY**: When analyzing any code review (GitHub, Copilot, human), extract ALL comments
    *   **Never assume** "suppressed" or "low confidence" means unimportant
    *   **User Quote Priority**: If user quotes specific review text I haven't mentioned, immediately acknowledge the miss
    *   **Complete Analysis Format**:
        - Total comments: X (Y visible, Z suppressed)
        - List EVERY comment with status (Addressed/Pending)
        - Explicitly search for hidden/suppressed comments
    *   **Cross-File Consistency**: When multiple files serve similar purposes, verify configuration matches

27. **Configuration Synchronization:**
    *   **When creating related files** with similar settings, always:
        1. Add sync comments: `# Keep in sync with other_file.py`
        2. Consider extracting to shared configuration file
        3. Verify lists/constants match across files
    *   **Prevention**: Avoids drift between related configurations that should stay aligned

25. **Dead Code Detection Protocol:**
    *   **MANDATORY TOOL**: Always use `vulture` for dead code detection instead of basic grep searches
    *   **Installation**: `pip install vulture` (install in virtual environment)
    *   **Basic Usage**: `vulture . --min-confidence 80 --exclude="*/venv/*,*/tests/*"`
    *   **Why Vulture**: Understands Python AST and detects:
        - Callback functions (e.g., `json.dumps(data, default=func)`)
        - Functions passed as parameters
        - Dynamic imports and getattr usage
        - Class methods and properties
    *   **Process**:
        1. Run vulture first to identify candidates
        2. Manually verify each finding (some may be false positives)
        3. Run tests before removing any code
        4. Remove incrementally and test after each removal
        5. Document findings in DEAD_CODE_ANALYSIS.md
    *   **Whitelist**: Create `.vulture_whitelist.py` for known false positives
    *   **Lesson Learned**: Basic grep missed `json_datetime_serializer` because it was used as a callback parameter

## III. Git & Repository Workflow

1.  **Source of Truth for Code is Main/Master Branch:**
    *   If I need to find an original or known-working version of a file, I **must** retrieve it from the `main` or `master` branch (e.g., via `git show main:<path/to/file>`). I will not ask you for it.

2.  **Establish Baseline:**
    *   I will assume we are operating in a large repository where the primary remote branch (`origin/main` or `origin/master`) is the last known stable state. If uncertain, I will ask.

3.  **Pre-Proposal Diff Review:**
    *   Before proposing changes, I will always review the cumulative diff against the merge-base of the target branch to verify the changes are accurate and safe.
    *   `git diff $(git merge-base origin/main HEAD) HEAD`

4.  **Repository Awareness:**
    *   When asked about the repository's state, I will inspect local Git logs and file diffs to provide informed answers.

5.  **Only Publish Working Code:**
    *   I must never push code to GitHub unless it has been verified to work locally first. This includes running relevant tests, ensuring the application starts without errors, and confirming core functionality works as expected.

6.  **Confirm Before Publishing:**
    *   After successfully committing changes, I will explicitly ask for your confirmation before I push them to the remote GitHub repository.

7.  **Provide Pull Request URL:**
    *   After successfully pushing a new branch with commits, I will provide the direct URL to create a pull request on GitHub.

8.  **Always Link Pull Request URLs:**
    *   When creating or referencing a pull request, I must always provide the direct clickable URL to the PR. This applies to both newly created PRs and when discussing existing ones.

9.  **Default to Main Branch for Pull Requests:**
    *   Unless explicitly instructed otherwise, all pull requests should target the `main` branch as the base. Do not assume `dev` or other branches without confirmation.

10. **Merge Protocol and Branch Management:**
    *   **CRITICAL**: Always use the `integrate` alias pattern for updating from main: `git checkout main && git pull && git branch -D dev && git checkout -b dev`
    *   **Main Branch Protection**: Never work directly on `main` - always use a local `dev` branch for protection
    *   **After Merges**: Always run the integrate pattern to get latest changes and create fresh dev branch
    *   **PR Creation Process**:
        1. Create feature branch from latest main using integrate pattern
        2. Make changes and commit with descriptive messages
        3. **MANDATORY**: Run `./run_tests.sh` and include test results in PR description
        4. Push branch and create PR with comprehensive description using `gh pr create`
        5. Provide clickable PR URL for user review
        6. After merge, immediately run integrate pattern before next PR
        7. **CRITICAL**: Never use 'dev' as a remote branch for PRs - always use descriptive feature branch names
    *   **PR Descriptions**: Always include Summary, Changes, Benefits, and Usage sections
    *   **Commit Messages**: Use descriptive titles with bullet points and Claude Code attribution

11. **GitHub CLI Preference for PR Operations:**
    *   Always use GitHub CLI (`gh`) commands for creating and managing pull requests when available
    *   This ensures proper authentication and streamlined workflow

12. **Detailed Progress Tracking for Milestones:**
    *   **Primary Tracking**: Main plan and progress in `roadmap/scratchpad_[remote_branch_name].md` as per Scratchpad Protocol above
    *   **Granular Tracking**: For complex projects with milestones and sub-bullets, create one progress file per sub-bullet
    *   **Progress Files**: Save to `tmp/milestone_X.Y_step_A.B_progress.json` with milestone, step, sub_bullet, status, timestamp, files_created, key_findings, and next_task
    *   **Atomic Commits**: One sub-bullet = one commit with descriptive message following the format:
        ```
        M{milestone} Step {step}.{sub_bullet}: {Brief description}
        
        - {Implementation detail 1}
        - {Key result or finding}
        - Saved progress to tmp/milestone_{milestone}_step_{step}.{sub_bullet}_progress.json
        
        ü§ñ Generated with [Claude Code](mdc:https:/claude.ai/code)
        Co-Authored-By: Claude <noreply@anthropic.com>
        ```
    *   **PR Updates**: Only update PR descriptions when local tests pass and significant milestones are reached
    *   **Force Add**: Use `git add -f tmp/*.json` for ignored progress files
    *   **Completion Tracking**: Update roadmap scratchpad file with ‚úÖ COMPLETED, üîÑ IN PROGRESS (X%), ‚¨ú NOT STARTED
    *   **Benefits**: Clear audit trail, easy recovery, detailed visibility, accurate estimates
    *   **Session Progress Tracking**: As mentioned in Scratchpad Protocol, also maintain `tmp/current_milestone_[branch_name].md` for session details
    *   **Purpose**: Enable seamless session resumption after interruptions by providing complete context

    *   When the user requests to apply/test a pull request, I must prefer using GitHub CLI commands over manual git operations.
    *   **Primary method**: Use `gh pr checkout <PR_NUMBER>` to apply PR changes locally for testing
    *   **Alternative methods**: If GitHub CLI is unavailable, use `git fetch origin pull/<PR_NUMBER>/head:<branch_name>` followed by `git checkout <branch_name>`
    *   This provides a more convenient and reliable workflow for PR testing and reduces the risk of manual errors in branch management.

12. **Branch Safety and Push Verification Protocol:**
    *   **CRITICAL**: Never accidentally push to main/master branch. Always verify branch and tracking before any push operation.
    *   **Pre-Push Checklist** (MANDATORY before every push):
        1. Check current branch: `git branch --show-current`
        2. Verify branch tracking: `git branch -vv`
        3. Confirm push target: Use explicit syntax `git push origin HEAD:branch-name`
        4. Test with dry-run first: `git push --dry-run`
    *   **Safe Branch Creation Pattern**: Always use this sequence to avoid tracking issues:
        ```bash
        git checkout main
        git pull origin main
        git checkout -b feature-branch-name
        # This creates an untracked branch, forcing explicit remote setup
        ```
    *   **NEVER use**: `git checkout origin/main -b branch-name` as it sets tracking to main
    *   **Push Safety**: Always use explicit push syntax: `git push origin HEAD:branch-name` instead of relying on `-u` or default tracking
    *   **Recovery Protocol**: If accidental main push is detected, immediately notify user and provide recovery options
    
13. **Pull Request Branch Verification Protocol:**
    *   **CRITICAL**: Before creating a pull request, ALWAYS verify that the branch contains only intended commits
    *   **Pre-PR Checklist** (MANDATORY before creating any PR):
        1. Check commits that will be included: `git log main..HEAD --oneline`
        2. Verify file changes: `git diff main...HEAD --name-only`
        3. Count commits: `git rev-list --count main..HEAD`
        4. If unrelated commits are found: WARN the user immediately
    *   **Warning Format**: "‚ö†Ô∏è WARNING: This branch contains X unrelated commits from other work. Creating a PR now will include all these changes. Recommend creating a fresh branch from latest main and cherry-picking only the intended commit(s)."
    *   **Clean Branch Pattern**: When warned about unrelated commits:
        ```bash
        git checkout main && git pull origin main
        git checkout -b clean-branch-name
        git cherry-pick <specific-commit-hash>
        ```
    *   **PR Creation**: Only proceed with PR creation after confirming the branch is clean or user explicitly approves including unrelated commits

14. **No Direct Push to Main Branch Protocol:**
    *   **ABSOLUTE PROHIBITION**: NEVER push directly to main/master branch under any circumstances
    *   **Git Push Commands**: When pushing, ALWAYS specify the target branch explicitly:
        - ‚úÖ CORRECT: `git push origin HEAD:feature-branch-name`
        - ‚úÖ CORRECT: `git push origin feature-branch:feature-branch`
        - ‚ùå FORBIDDEN: `git push origin main` or any variant that targets main
        - ‚ùå FORBIDDEN: `git push` without explicit branch specification if tracking main
    *   **Accidental Push Prevention**:
        1. Always verify current branch before pushing: `git branch --show-current`
        2. Always use explicit branch syntax in push commands
        3. If a push accidentally goes to main, immediately alert the user
    *   **PR-Only Workflow**: All changes to main must go through pull requests - no exceptions
    *   **Recovery**: If accidental push to main occurs, immediately:
        1. Stop all operations
        2. Alert user with: "‚ö†Ô∏è CRITICAL: Accidentally pushed to main branch!"
        3. Provide revert instructions if needed

## IV. Environment, Tooling & Scripts

1.  **Python Virtual Environment Management:**
    *   I will verify that the project-specific virtual environment (`venv`) is activated before running any Python scripts, linters, testers, or package managers. If it's not active, I will attempt to activate it or inform you if I cannot.
2.  **Write Robust & Context-Aware Scripts:**
    *   Automation scripts (e.g., `deploy.sh`) will be designed to be robust, idempotent, and work correctly from any subdirectory.
3.  **MANDATORY Python Execution Protocol - Always Run From Project Root:**
    *   **CRITICAL RULE**: ALL Python commands MUST be executed from the project root directory. This is NON-NEGOTIABLE.
    *   **WHY**: Python's import system requires consistent execution context. Running from subdirectories breaks relative imports and package structures.
    *   **ENFORCEMENT**: Before ANY Python command, I MUST verify I'm at project root with `pwd`. If not at root, I MUST navigate there first.
    *   **PROHIBITED**: Never run Python files from subdirectories like `cd prototype && python3 file.py` or `cd mvp_site && python3 test.py`
    *   **CORRECT PATTERN**: Always use full paths from project root:
        - `python3 prototype/some_file.py` ‚úì
        - `vpython mvp_site/test_file.py` ‚úì
        - `TESTING=true vpython mvp_site/test_integration.py` ‚úì
    *   **IMPORT PATTERN**: When imports fail, the solution is ALWAYS to run from project root, NOT to modify imports.
4.  **Use `vpython` for Tests - Consistent Execution Pattern:**
    *   Always use `vpython` to run tests when available. If `vpython` is not available, use `python3` but ALWAYS from project root.
    *   **CRITICAL: When user says "run all tests", always use `./run_tests.sh` script from project root instead of manual unittest commands.**
    *   **CRITICAL: When ANY test fails, I must either fix it immediately or explicitly ask the user if it should be fixed. I must highlight the entire line of failing test output in red for visibility.**
    *   **UPDATED Directory Navigation for `vpython`:**
        - **ALWAYS from project root**: `TESTING=true vpython mvp_site/test_file.py`
        - **NEVER cd into subdirectories** for Python execution
        - If unsure of location: Use `pwd` first, then navigate to project root
        - For prototype tests: `vpython test_prototype_working.py` (from root)
    *   **Correct Test Commands:** 
        - Integration tests: `TESTING=true vpython mvp_site/test_integration.py`
        - Specific tests: `vpython -m unittest mvp_site.test_module.TestClass.test_method`
        - Prototype tests: `python3 test_prototype_working.py`
4.  **Tool Failure and Recovery Protocol:**
    *   If a command or tool fails more than once, I must stop and try an alternative command or a different approach. I will not repeatedly attempt the same failing action. If a file becomes corrupted or its state is uncertain due to failed edits, my default recovery strategy is to fetch the last known good version from the `main` or `master` branch and restart the editing process.
5.  **Use Full-Content Tools for Web Scraping:**
    *   When the goal is to download the content of a webpage, I must use a tool that retrieves the full page content (e.g., `curl`). I will not use a search tool (like `web_search`) that only returns snippets, as the primary objective is to acquire the complete text.

## V. Knowledge Management & Process Improvement

This protocol uses a set of files to manage our workflow. The primary tracking happens in `roadmap/scratchpad_[remote_branch_name].md` as defined in the Scratchpad Protocol above. Additional files in the `.cursor` directory at the project's root provide supplementary tracking. If they don't exist, I will create them. I will review them before each interaction and update them after.

2.  **.cursor/rules/lessons.mdc - Persistent Learnings:**
    *   **Purpose:** A persistent, repository-agnostic knowledge base for reusable techniques, best practices, and insights.
    *   **Workflow:** When we solve a novel problem or I am corrected, I will document the actionable learning here to avoid repeating past mistakes.

3.  **.cursor/project.md - Project-Specific Knowledge Base:**
    *   **Purpose:** A technical knowledge base for *this specific repository*.
    *   **Workflow:** As I work on files, I will document their functionality, APIs, and the "dependency graph" relevant to my tasks to build a focused, evolving design document of the areas I've engaged with.

4.  **"5 Whys" for All Corrections and Failures:**
    *   When a significant error occurs, or whenever you correct a mistake in my process or code, I **must** perform a root cause analysis. The resulting "Actionable Lesson" **must** be documented in `.cursor/rules/lessons.mdc` to prevent that class of error in the future.

5.  **Synchronize with Cursor Settings:**
    *   After we modify this `rules.mdc` file, I will remind you to copy its contents into the "Edit an AI Rule" section of the Cursor settings to ensure my behavior reflects the most current protocol.

6.  **Proactive Rule and Lesson Documentation:**
    *   After completing any significant debugging session, integration test work, or bug fixes, I must proactively update both `.cursor/rules/rules.mdc` and `.cursor/rules/lessons.mdc` with relevant lessons learned, without waiting for explicit instruction.
    *   This ensures knowledge preservation and prevents repeating the same mistakes in future sessions.

## VI. Project-Specific Lessons Log

*This log captures key technical decisions and fixes from our sessions.*

*   **Flask SPA Routing:** A Flask backend serving a SPA must have a catch-all route to serve `index.html` for all non-API paths.
*   **CSS/JS Caching:** To avoid stale static assets during development, restart the dev server and perform a hard refresh in the browser. Cache-busting techniques (e.g., query params) are best for production.
*   **Python `venv` & PEP 668:** To avoid system package conflicts (`externally managed`), always work within a project-specific virtual environment. On some systems, `python3-venv` may need to be installed via the system package manager first.
*   **Shell Config (`.bashrc`):** Changes to shell configs require sourcing the file (e.g., `source ~/.bashrc`) or starting a new session to take effect.
*   **LLM System Prompts:** Detailed, explicit, and well-structured system prompts are crucial for improving AI performance and consistency.
*   **Dotfile Backups:** Critical configuration files in transient environments (like Cloud Shell) should be version-controlled or backed up.

- After implementing a feature or a fix, I should always offer to run the relevant unit tests to verify the changes.

- **Before attempting a complex solution, such as refactoring code or changing core logic, first explicitly state and evaluate the simplest possible solution that could achieve the user's goal.** This forces a "simple-first" approach, prioritizing the most direct path to the objective over unnecessary refactoring.

## VII. Data Integrity and AI Management

1.  **Prioritize Data Integrity:** When handling data from any source (database, API, AI), I must assume the data may be incomplete or malformed. I will defensively access all dictionary keys and object properties (e.g., using `dict.get()` or optional chaining) and validate data structures before processing them.
2.  **Enforce Critical Logic in Code:** For operations that are critical to data integrity (like appending to a log or preventing state corruption), I will always implement safeguards and validation in the application code rather than relying solely on instructing an AI through prompts.
3.  **Verify Data Paths:** When investigating bugs related to data not being saved, my first step will be to verify and log the full read path and the full write path to ensure they are identical.
4.  **Maintain a Single Source of Truth for AI Instructions:** When creating or modifying instructional documents for an AI (e.g., prompt files), I must ensure there is one, and only one, clear and unambiguous way to perform a given task. I will remove or refactor any conflicting examples or rules.
5.  **Always Check for File Existence Before Creating:** Before writing to a file that I believe might be new (like a configuration or documentation file), I must first use a command like `ls` to verify whether it already exists. If it does, I must read it and append to it, rather than overwriting it.

## VIII. User Communication & Formatting

1.  **Raw Markdown for "Markdown Format":** When you ask for output in "markdown format" (e.g., for a pull request description), I will provide the raw, unrendered Markdown text enclosed in a ` ```markdown ... ``` ` code block. This ensures the content is easy for you to copy and paste directly.

## IX. Code Review and Integration Validation

### Critical Review Process
When reviewing code changes or merges, ALWAYS follow this validation process:

1. **Validate AI Instructions Have Implementation**: Always verify that documented AI capabilities (especially in `prompts/` directory) have corresponding code implementation
2. **Trace Data Flow End-to-End**: For any AI-driven feature, trace the complete pipeline: AI instruction ‚Üí parsing ‚Üí state updates ‚Üí storage
3. **Integration Testing Over Syntax**: Treat reviews as functional integration tests, not just code quality checks
4. **Search for Missing Implementations**: Before approving merges, search codebase for referenced tokens/features (e.g., `__DELETE__`, special syntax, documented functions)

### Common Pitfalls to Avoid
- **Never assume documentation equals working implementation** - Always verify
- Focusing on code structure without validating complete data pipelines
- Treating AI instructions as truth rather than specifications to verify
- Missing gaps between what AI is instructed to do vs what code actually processes

### Validation Commands
```bash
# Search for documented features in prompts
grep -r "__DELETE__" prompts/
grep -r "special_token" prompts/

# Verify implementation exists
grep -r "__DELETE__" *.py
grep -r "process.*special_token" *.py
```

**Critical Lesson**: A bug was missed where AI was instructed to use `__DELETE__` tokens for defeated enemies, but no code existed to process these tokens, causing combat state inconsistencies. This type of documentation-implementation gap must be caught during review.

## X. Critical Lessons and Rules

### Automatic Rule Updates
**MANDATORY**: Whenever I make a mistake, encounter a bug I should have caught, or receive correction from the user, I MUST immediately update both this rules.mdc file and .cursor/rules/lessons.mdc with the lesson learned. I will not wait for the user to remind me - this is an automatic responsibility that happens every time I fail or am corrected.

### Temporary Fix Protocol - NEVER GLOSS OVER
**CRITICAL RULE**: When implementing ANY temporary fix or workaround:
1. **IMMEDIATELY flag it** - "‚ö†Ô∏è TEMPORARY FIX: This will break when [specific scenario]"
2. **PROPOSE permanent solution in same message** - Don't wait to be asked
3. **Run the checklist**:
   - [ ] Will this work from a fresh clone?
   - [ ] Will this work in CI/CD?
   - [ ] Will this work for other developers?
   - [ ] Will this work next week/month?
   - [ ] What are ALL the failure scenarios?
4. **Create the permanent fix NOW** - Not "we could fix it" but actually implement it
5. **Document assumptions** - "This assumes [X] which will fail if [Y]"

**Example**: When copying files manually for deployment:
- ‚ùå BAD: "I copied the files, deployment works now"
- ‚úÖ GOOD: "‚ö†Ô∏è TEMPORARY FIX: I manually copied world/ to fix deployment. This WILL BREAK on next deploy from fresh clone. Creating permanent fix to deploy.sh now..."

**Lesson**: Manually copied world directory for deployment without immediately fixing deploy.sh, causing future deployment failures. Always think about sustainability, not just immediate success.

### Task Completion Protocol (December 2024)
**CRITICAL REDEFINITION**: Task completion is NOT just solving the user's immediate problem. Task completion includes all mandatory follow-up actions as core requirements.

**NEW TASK COMPLETION DEFINITION**: A task is only complete when ALL of the following steps are finished:
1. ‚úÖ **Solve user's immediate problem** (the obvious part)
2. ‚úÖ **Update .cursor/rules/lessons.mdc** (for any error resolution, bug fix, or correction)
3. ‚úÖ **Update memory with lesson learned** (to prevent immediate recurrence)
4. ‚úÖ **Self-audit compliance with all documented procedures** (verify I followed all protocols)
5. ‚úÖ **Consider task truly complete** (only after all above steps)

**MANDATORY COMPLETION CHECKLIST**: For every error resolution, bug fix, or user correction, I MUST follow this checklist:
- [ ] Problem solved to user satisfaction
- [ ] Lessons documented in .cursor/rules/lessons.mdc
- [ ] Memory updated with prevention strategy  
- [ ] Self-audit: "Did I follow all mandatory procedures?"
- [ ] Task marked complete only after ALL steps finished

**SELF-AUDITING REQUIREMENT**: At the end of every significant interaction, I must explicitly ask myself: "Did I follow all mandatory procedures?" This is not optional - it's part of systematic process discipline.

**ENFORCEMENT PRINCIPLE**: Documentation is part of the solution, not administrative overhead. Any error resolution that doesn't include lessons capture is an incomplete solution that increases the risk of recurrence.

### Meta-Rule Violation: Lessons Documentation Failure
**CRITICAL RULE VIOLATION DOCUMENTED (December 2024)**: After completing a full 10 Whys analysis of the enhanced components failure, I initially failed to automatically update lessons.mdc as required. This is a **direct violation** of the core Automatic Rule Updates mandate.

**ROOT CAUSE**: Lack of systematic process discipline - I operate without internal mechanisms to ensure compliance with documented procedures, leading to selective rule following and task completion blindness.

**ENFORCEMENT**: Failure to document lessons after mistakes is itself a critical failure that compounds the original problem. The lessons documentation requirement is **NOT OPTIONAL** and must happen immediately, automatically, every time - without user prompting.

### Data Corruption Pattern Analysis
**CRITICAL RULE**: When encountering ANY data corruption bug, treat it as a systemic issue requiring comprehensive pattern analysis:
- Search for ALL similar corruption patterns across the codebase (e.g., `str()` conversions, type changes)
- Identify ALL code paths that process the same data structures
- Apply the principle: "If there's one bug of this type, there are likely others"
- Create data integrity audit checklists for similar data structures

**Lesson**: Missed NPC data corruption because I focused on isolated `__DELETE__` bug without auditing all `str()` conversions that could corrupt structured data.

### AI Instruction Priority and Ordering
**CRITICAL RULE**: When AI systems have multiple competing instructions, instruction ORDER determines compliance:
- **Most critical instructions MUST be loaded first** (e.g., state management, core protocols)
- Later instructions can override or distract from earlier ones
- Long instruction sets suffer from "instruction fatigue" where later rules are ignored
- Always prioritize core functionality over stylistic preferences in prompt ordering

**Lesson**: AI was ignoring state update requirements because game state instructions were loaded LAST after lengthy narrative instructions. Moving them FIRST fixed the core state update failure.

### Test Truth and Architectural Validation
**CRITICAL RULE**: Test names and claims MUST match what is actually being tested:
- **Import Verification**: Always verify and log which module is being imported in tests
- **Dependency Presence**: Check that required dependencies exist before claiming approach works
- **Negative Testing**: Test validation REJECTION cases, not just success cases
- **Test-First Architecture**: Write validation tests BEFORE making architectural decisions

**Enforcement**:
1. Add module verification assertions: `assert 'pydantic' in str(Model.__module__)`
2. Log test metadata: what's being tested, which modules, what dependencies
3. Create Architecture Decision Tests (ADTs) that verify decisions remain valid
4. Never trust test names - verify the implementation matches the claim

**Lesson**: We made critical architectural decisions based on "Pydantic" tests that were actually testing a non-Pydantic implementation, leading to a cascade of errors.

### Data Structure Schema Enforcement 
**CRITICAL RULE**: When AI systems generate structured data, explicit schemas with mandatory data types must be enforced through prompt engineering:
- **Schema Definition**: Clearly define expected data structures (lists vs dictionaries, required fields)
- **Contradictory Instructions**: Remove any conflicting examples or deprecated patterns that confuse the AI
- **Type Validation**: Add explicit rules forbidding type changes (e.g., "active_missions MUST be a list, never a dictionary")
- **Example Clarity**: Provide concrete examples of correct vs incorrect formatting

**Lesson**: Fixed critical data corruption where AI was changing `active_missions` from list to dictionary and using incorrect `combat_state` schemas by adding explicit data type rules and removing contradictory dot notation examples.

### Codebase Exploration Protocol
**CRITICAL RULE**: When working with any codebase, ALWAYS follow this sequence:
1. **Run tests FIRST** - before reading any code, run the full test suite to establish baseline understanding of what works/breaks
2. **Read project documentation** - Review CLAUDE.md, README, and project instructions
3. **Investigate specific issues** - Only then dive into individual files and components

**Lesson**: Failed to catch multiple dependency and API inconsistency bugs because I explored code files individually instead of running tests first. The test suite immediately reveals: current state, expected behavior, dependency issues, and API changes. This systematic approach prevents iterative discovery of issues that should be obvious upfront.

### Integration Before Testing Protocol
**CRITICAL RULE**: When asked to test or validate any feature:
1. **STOP and verify prerequisites** - Check if the feature is actually integrated and functional
2. **Question test readiness** - Ask "Is this code connected to the main flow?"
3. **Propose integration first** - If components aren't wired up, suggest integration before testing
4. **Explain dependencies** - Clearly state why integration is needed for meaningful results
5. **Never test disconnected code** - Testing unintegrated modules wastes time and gives false results

**Lesson**: Testing validation approaches without integrated mitigation strategies provided meaningless results (0% success for both approaches). Always ensure code is connected before testing.

### Task Analysis Protocol
**CRITICAL RULE**: When receiving any user request:
1. **Analyze before executing** - Understand the full context and purpose
2. **Check assumptions** - "What needs to be true for this task to be meaningful?"
3. **Identify dependencies** - "What other components must be working?"
4. **Communicate gaps** - Immediately inform user of missing prerequisites
5. **Propose correct sequence** - Suggest the right order of operations

**Example**: User says "run tests" ‚Üí Check if feature is integrated ‚Üí If not, say "The feature needs integration first. Should I integrate it before testing?"

### Execution Mode vs Analysis Mode
**CRITICAL RULE**: Never let "execution mode" override critical thinking:
- **Red flags that you're in pure execution mode**:
  - Implementing exactly what's asked without questioning feasibility
  - Continuing despite discovering blockers
  - Prioritizing "showing progress" over "achieving goals"
  - Running tests that you know will fail
- **Required mindset**: Every task requires both analysis AND execution
- **Recovery**: If you catch yourself in pure execution mode, STOP and reassess

**Lesson**: Blindly executed test comparisons without integrating the code being tested, wasting time on meaningless results. Discovered entity tracking modules weren't connected but continued testing anyway.

## XI. API Error Prevention

**CRITICAL RULE: Do Not Print Code or File Content**
- **NEVER** include actual code text, file content, or long text blocks in responses
- **API Error Risk**: Large text blocks or code snippets cause "empty message" API errors  
- **Status Updates Only**: Provide concise status updates about what was changed/fixed
- **Reference by Line Numbers**: Use file_path:line_number format instead of showing code
- **Summary Over Details**: Describe what was done, not the actual implementation

**Root Cause**: I cannot control API request/response handling or message filtering. When large content gets filtered out, the final message to Claude API becomes empty, triggering "all messages must have non-empty content" errors.

**Mitigations I Can Control**:
- Keep all responses concise and avoid large text blocks
- Use file references (file_path:line_number) instead of printing actual code
- Provide status updates rather than detailed content reproduction
- Never include full file contents, code snippets, or extensive quoted material
- Limit responses to essential information only

**What I Cannot Control**: API request/response handling, message filtering/processing, or the actual API error mechanism itself.

## XII. User Command Shortcuts

### Context Estimation Command
**Triggers**: "est", "estimate context", "context usage", "how much context left"

**Response Format**: When user requests context estimation, provide:
1. **Session Context Usage**: Estimated percentage of my context window used
2. **Breakdown by Category**:
   - System messages & instructions: ~X%
   - File reading operations: ~X%
   - Conversation history: ~X%
   - Tool outputs & responses: ~X%
3. **Remaining Capacity**: Percentage and practical limitations
4. **Usage Indicators**: What suggests approaching limits
5. **Recommendations**: Whether to continue or start fresh session

**Example Response**:
```
Session Context Usage: ~75-85% used
- System messages: ~10-15%
- File operations: ~30-40%  
- Conversation: ~25-35%
- Tool outputs: ~10-15%

Remaining: ~15-25% (good for a few more operations)
Recommendation: Approaching limits, consider fresh session for major work
```

## XIII. Lessons Archive Process

### When to Archive
- Archive lessons quarterly (January, April, July, October)
- Archive when lessons.mdc exceeds 2500 lines
- Archive when switching to a new major version or year

### Archive Process
1. Create archive file: `lessons_archive_YYYY.mdc` (e.g., lessons_archive_2024.mdc)
2. Move all lessons older than 30 days to archive
3. Keep CRITICAL patterns as generalized rules in main file
4. Add reference in lessons.mdc header pointing to archive

### Archive Structure
```
lessons_archive_YYYY.mdc
‚îú‚îÄ‚îÄ Q1 (January-March)
‚îú‚îÄ‚îÄ Q2 (April-June)
‚îú‚îÄ‚îÄ Q3 (July-September)
‚îî‚îÄ‚îÄ Q4 (October-December)
```

### Referencing Archived Content
- Add "See also: lessons_archive_YYYY.mdc#section" for related patterns
- Maintain searchability across all archive files
- Never delete archives - they contain valuable historical context

## XIV. Deployment

- Multi-environment deployment via `./deploy.sh` script
- Docker containerization with health checks and 300s timeout
- Google Cloud Run with secret management for API keys
- Automatic context-aware deployment from any directory

