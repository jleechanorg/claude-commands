---
description: 
globs: 
alwaysApply: true
---
# Core Operating Protocol for AI Collaboration

**PRIMARY RULE SOURCE: This file (.cursor/rules/rules.mdc) is the single source of truth for all operating protocols, development guidelines, and project-specific rules. CLAUDE.md should only contain Claude Code specific behavior.**

**Meta-Rule: Before beginning any task, you must check for the existence of a .cursor/rules/rules.mdc file in the workspace root. If this file exists, you must read it and treat its contents as your primary operating protocol, superseding any other general instructions.**

** anchoring rule:** The `.cursor` directory at the absolute top level of the workspace is the single source of truth for all protocol and lessons files. Any instruction to modify rules, lessons, or project documentation refers *exclusively* to the files within this top-level `.cursor` directory (e.g., `.cursor/rules/rules.mdc`). I will not create, read, or write rule files in any other location.

**VIRTUAL AGENTS APPROACH: This project uses a True Multi-Agent Development System as documented in `coding_prompts/virtual_agents.md`. See that file for complete details on the SUPERVISOR-WORKER-REVIEWER pattern and when to use single agent vs three-agent approach.**

This document outlines the operating protocol for our collaboration. It merges general best practices with specific lessons learned from our work on this project.

## Project Overview

WorldArchitect.AI is an AI-powered tabletop RPG platform that serves as a digital Game Master for D&D 5e experiences. The application uses Google's Gemini AI to generate dynamic narratives, manage game mechanics, and create interactive storytelling.

### Technology Stack

- **Backend**: Python 3.11 + Flask + Gunicorn
- **AI Service**: Google Gemini API (2.5-flash, 2.5-pro models) via `google-genai` SDK
- **Database**: Firebase Firestore for persistence and real-time sync
- **Frontend**: Vanilla JavaScript (ES6+) + Bootstrap 5.3.2
- **Deployment**: Docker + Google Cloud Run

### Architecture

#### Core Components

- **`main.py`**: Flask application entry point with API routes
- **`game_state.py`**: Campaign state management, combat system, data migrations
- **`gemini_service.py`**: AI integration with context management and personality-driven responses
- **`firestore_service.py`**: Database operations and real-time synchronization
- **`document_generator.py`**: Export campaigns to PDF/DOCX/TXT formats

#### AI System

The platform uses three specialized AI personas:
- **Narrative Flair**: Storytelling and character development
- **Mechanical Precision**: Rules and game mechanics  
- **Calibration Rigor**: Game balance and design

System instructions are stored in `prompts/` directory with MBTI personality definitions in `prompts/personalities/`.

#### Data Architecture

- **Firestore Collections**: Campaigns, game states, user data
- **State Management**: Deep recursive merging for nested updates
- **Schema Evolution**: Defensive data access patterns for backward compatibility
- **Append-Only Patterns**: Game history preserved through append operations

### File Structure

```
mvp_site/                    # Main application
├── main.py                  # Flask routes and application factory
├── game_state.py           # Core state management
├── gemini_service.py       # AI service integration  
├── firestore_service.py    # Database layer
├── static/                 # Frontend assets (app.js, auth.js, style.css)
├── prompts/                # AI system instructions
├── test_*.py              # Comprehensive test suite
└── requirements.txt        # Python dependencies
```

### Development Commands

#### Testing
```bash
# CRITICAL: When user says "run all tests", always use run_tests.sh script from project root
./run_tests.sh

# CRITICAL: When ANY test fails, either fix it immediately or explicitly ask user if it should be fixed

# Run specific test file
cd mvp_site && TESTING=true vpython test_integration.py

# Run data integrity tests (catches corruption bugs)
cd mvp_site && TESTING=true vpython test_data_integrity.py

# Run combat integration tests (end-to-end combat flow)
cd mvp_site && TESTING=true vpython test_combat_integration.py

# Run specific test method
cd mvp_site && TESTING=true vpython -m unittest test_module.TestClass.test_method
```

#### Running the Application
```bash
# Development (from mvp_site directory)
cd mvp_site && vpython main.py

# Production (Docker)
./deploy.sh                 # Deploy to dev environment
./deploy.sh stable          # Deploy to stable environment
```

#### Environment Setup
- Always use the project virtual environment (`venv`)
- Set `TESTING=true` environment variable when running tests (uses faster AI models)
- Firebase service account configured via environment variables

### Important Development Patterns

#### Gemini API Usage
```python
# Use modern google-genai SDK patterns
import google.generativeai as genai
client = genai.Client()
response = client.models.generate_content(model=MODEL_NAME, contents=prompt)
```

#### Data Handling
```python
# Defensive data access for schema evolution
campaign_data.get('last_played', 'N/A')  # Backend
campaign?.last_played || 'N/A'           # Frontend

# Type validation for dynamic data
if isinstance(item, dict):
    name = item.get('name', str(item))
```

#### Testing Patterns
- Use `TESTING=true` environment variable for faster AI models
- Always use temporary directories for test file operations
- Case-insensitive field detection for AI-generated data
- Flexible assertions for AI content validation

### Key Constraints

- **AI Models**: Current models are `DEFAULT_MODEL = 'gemini-2.5-flash'`, `LARGE_CONTEXT_MODEL = 'gemini-2.5-pro'`, `TEST_MODEL = 'gemini-1.5-flash'` - never change without explicit user authorization
- **SDK**: Must use modern `google-genai` SDK patterns (not legacy `google-generativeai`)
- **Virtual Environment**: Always activate `venv` before running Python commands
- **Test Isolation**: Use temporary directories to avoid overwriting application files
- **Data Integrity**: Implement defensive programming for external data sources

## I. Core Principles & Interaction

1.  **Clarify and Understand the Goal:**
    *   Before initiating any work, I will ensure I have a complete and unambiguous understanding of your task. If anything is unclear, I will ask for clarification immediately.

2.  **Your Instructions are Law:**
    *   Your explicit instructions regarding code, component names, and file contents are the absolute source of truth.

3.  **Deletion is Prohibited Without Explicit Confirmation:**
    *   I am strictly prohibited from deleting any asset—be it a file, directory, or lines of code—without first proposing the exact deletion and receiving explicit, affirmative confirmation from you. My default operation must always be to preserve and add, never to remove, without your consent.

4.  **Leave Working Code Alone & Adhere to Protocol:**
    *   I will not modify functional code to satisfy linters or for any other non-essential reason without your explicit permission.
    *   I will review these rules before every response to ensure I am in full compliance.

5.  **Focus on the Primary Goal:**
    *   I will prioritize the user's most recent, explicit goal above all else. I must not get sidetracked by secondary issues like linter warnings or unrelated code cleanup unless explicitly instructed to do so.

6.  **Propose and Confirm:**
    *   My primary mode of operation is to propose a solution for your confirmation before implementing it, especially for complex changes.

7.  **Acknowledge Key Takeaways:**
    *   I will summarize important points after major steps or debugging sessions to ensure we are aligned.

8.  **Externalize Knowledge for Transparency:**
    *   As a core directive, I recognize that you have no access to my internal memories. Therefore, all important rules, project-specific knowledge, and lessons learned **must** be externalized into the appropriate files within the `.cursor/` directory (`rules/rules.mdc`, `rules/lessons.mdc`, `project.md`).

9.  **Rule Synchronization Protocol:**
    *   Any instruction to "add a rule," "change the rules," or a similar directive refers specifically to this file: `.cursor/rules/rules.mdc`.
    *   When such a directive is given, I **must** update both my internal memories and this file in parallel.
    *   If the directive is ambiguous in any way, I must ask for clarification before proceeding.

10. **General Principles Over Specific Details:**
    *   Both `.cursor/rules/rules.mdc` and `CLAUDE.md` must contain **general principles and protocols** only
    *   **Specific technical failures, code patterns, and detailed incident analysis** belong in `.cursor/rules/lessons.mdc`
    *   **Rules files** establish timeless operational principles; **lessons files** capture specific technical learning
    *   When adding content, ask: "Is this a general principle or a specific technical detail?" and place accordingly

11. **Verify Edits Before Concluding:**
    *   After every file edit, I must not assume the change was applied correctly. I will use my tools (`git diff` or `read_file`) to verify that the change exactly matches my intention before proceeding. This is especially critical for deletions, reverts, and changes to rule files.

12. **Ignore Firestore Linter Errors:**
    *   I will disregard any linter errors originating from Firebase/Firestore code and assume the code is functional unless you instruct me otherwise.

13. **Edits Must Be Additive or Surgical:**
    *   When modifying a file, my changes must be strictly additive (adding new code) or surgical (modifying specific lines). I must never delete existing, unrelated code, especially tests. I will verify this by carefully inspecting the diff after every `edit_file` operation.

14. **Propose Failing Tests:**
    *   When debugging a bug that is not covered by the existing test suite, I must first propose a new unit test that reliably reproduces the failure (a "red" test). I will explain why the test is expected to fail and will not add it to the code until you explicitly approve it.

15. **Red-Green Testing Methodology:**
    *   When implementing new features or functionality, I must follow the red-green testing approach: First, write a unit test that fails without the new implementation ("red" state), then implement the feature to make the test pass ("green" state). This ensures the test actually validates the new functionality and catches regressions.

16. **Security: DO NOT SUBMIT Rule:**
    *   If I see "DO NOT SUBMIT" anywhere in code, comments, or environment variables, I must never allow this to be committed or merged into any branch. This is a critical security marker indicating sensitive data (API keys, credentials, etc.) that must be removed before any PR or commit.

## II. Development, Coding & Architecture

1.  **Preservation Over Efficiency:**
    *   My most critical coding directive is to treat existing code as a fixed template. I will make surgical edits and will not delete or refactor working code without your explicit permission.

2.  **String Constant Management:**
    *   To improve maintainability and prevent errors from typos, string literals must be managed as constants.
    *   **Rule:** If a string literal is used more than once within a single file, it must be defined as a module-level constant (e.g., `MY_CONSTANT = "my_string"`).
    *   **Rule:** If a string literal is used across multiple files (e.g., as a dictionary key in a data structure), it must be defined in the global `constants.py` file and imported where needed.

3.  **DRY (Don't Repeat Yourself):**
    *   Code used in multiple places should be refactored into a helper function to improve maintainability.

4.  **Robust Data Handling:**
    *   Application code must be resilient to data variations, handle API limits gracefully, and manage different data schemas within the application logic.

5.  **Case-Insensitive Field Detection:**
    *   When searching for fields that may have inconsistent naming (e.g., AI-generated data), implement case-insensitive searches and handle multiple possible field names (e.g., `hp_current`, `HP_current`, `hp`).

6.  **Defensive Data Structure Handling:**
    *   Before calling methods on data structures, always verify the data type. Collections from external sources may contain mixed types (strings, dictionaries, etc.) that require different handling approaches.

7.  **Professional-Grade Development Practices:**
    *   I will follow standard best practices, including: using the `logging` module in Python, using docstrings, and ensuring all DOM-manipulating JavaScript is properly loaded.

8.  **Verify, Don't Assume:**
    *   I will use my tools to check the current state of the codebase (e.g., API method signatures, library versions) before making assumptions.

9.  **Use the Correct Gemini SDK:**
    *   This project uses the modern `google-genai` Python SDK. All Gemini API calls **must** conform to the patterns in the official migration guide: [https://ai.google.dev/gemini-api/docs/migrate](mdc:https:/ai.google.dev/gemini-api/docs/migrate). This means using `genai.Client()` for initialization and `client.models.generate_content()` for API requests. I will not use the legacy `genai.GenerativeModel()` pattern.

10. **Do Not Change the AI Model:**
    *   **AI Models**: Current models are `DEFAULT_MODEL = 'gemini-2.5-flash'`, `LARGE_CONTEXT_MODEL = 'gemini-2.5-pro'`, `TEST_MODEL = 'gemini-1.5-flash'` - never change without explicit user authorization

11. **Snippet-Based Code Modification:**
    *   By default, I will provide targeted code snippets with precise instructions on where to integrate them, rather than replacing entire files.

12. **No Unsolicited Refactoring:**
    *   I will not perform any cleanup, refactoring, or other changes that are not directly part of the assigned task. I may suggest these changes, but I must await your explicit approval before implementing them.

13. **Always Use Temporary Files for Testing:**
    *   When creating test files or any files that might overwrite user data, I must always use temporary directories (e.g., `tempfile.mkdtemp()`) to avoid overwriting real project files. This prevents data loss and ensures test isolation.

14. **Defensive Programming for Dynamic Data:**
    *   When processing data from external sources (databases, APIs, AI responses), I must always validate data types before attempting operations. Use `isinstance(data, expected_type)` checks before calling type-specific methods like `.get()`, `.append()`, etc.
    *   **Pattern:** For dictionary iteration, always check `if isinstance(item, dict):` before using dictionary methods.

15. **Integration Test Design Principles:**
    *   Integration tests should work with natural application state rather than pre-setting conflicting test data.
    *   Use flexible assertions that verify behavior rather than exact values when testing AI-generated content.
    *   Optimize test speed by using faster AI models (`gemini-1.5-flash`) and shared test fixtures when possible.
    *   Always use proper temporary directories to avoid overwriting real application files during testing.

16. **Global Event Listener Safety Protocol:**
    *   **PROHIBITION**: Never use `document.addEventListener('click', ...)` or other global DOM event listeners without explicit user approval
    *   **REQUIREMENT**: Always use targeted event binding to specific elements instead of document-wide listeners
    *   **ANALYSIS**: Any global listener must include detailed justification of why targeted binding is insufficient
    *   **PATTERN**: Use `element.addEventListener()` or delegate only within specific containers (`container.addEventListener`)
    *   **SELECTOR SAFETY**: Avoid broad CSS selectors that might match unintended elements (e.g., `[data-theme]` matching document root)
    *   **EXCEPTION**: Only use global listeners for genuinely global concerns (keyboard shortcuts, window resize, system-level events)

17. **Cross-Cutting Concern Integration Analysis:**
    *   **MANDATORY**: Before implementing any feature that modifies DOM, CSS, or JavaScript, explicitly categorize the change:
        - **Surface Feature**: Only affects specific UI components (safe)
        - **Cross-Cutting Concern**: Touches multiple systems (requires integration analysis)
        - **Infrastructure Change**: Modifies core application behavior (very dangerous)
    *   **FOR Cross-Cutting Concerns**: Mandatory integration analysis of ALL existing systems before implementation
    *   **ANALYSIS REQUIRED**: Event handling patterns, DOM manipulation scope, CSS cascade impact, global state changes
    *   **BLAST RADIUS**: Document all systems the change will touch before proceeding

18. **Core User Workflow Protection:**
    *   **IDENTIFICATION**: Before implementing any system modification, identify the top 3-5 core user workflows
    *   **CORE WORKFLOWS**: Authentication, Campaign Creation, Game Interaction, Data Persistence, Form Submissions
    *   **MANDATORY TESTING**: Test each core workflow after ANY system modification, regardless of perceived scope
    *   **VALIDATION**: All core workflows must function identically before and after feature addition
    *   **ROLLBACK TRIGGER**: ANY core workflow failure requires immediate feature rollback

19. **File Naming Conventions:**
    *   **No Red/Green in Filenames:** I must never use "red", "green", "red_green", or similar color-based terminology in filenames. This avoids confusion and maintains professional naming standards.
    *   Use descriptive names that indicate the file's purpose instead (e.g., `test_comprehensive.py`, `test_integration.py`, `test_edge_cases.py`).

20. **Backup and Temporary File Management:**
    *   **MANDATORY**: All backup files (*.backup, *.orig, *.old) and temporary files (temp_*, *_temp, *_tmp) MUST be placed in the `tmp/` directory within each module
    *   **Structure**: Each module/directory should have its own `tmp/` subdirectory for organizing backups and temporary files
    *   **Never Leave Scattered**: Never leave backup or temporary files scattered throughout the codebase - always consolidate into appropriate tmp/ directories
    *   **Naming Convention**: Use descriptive names that indicate the original file and purpose (e.g., `narrative_system_instruction.md.backup`, `temp_header.txt`)
    *   **Git Tracking**: Backup files in tmp/ directories may be committed for safety, but temporary working files should generally be excluded via .gitignore

## III. Git & Repository Workflow

1.  **Source of Truth for Code is Main/Master Branch:**
    *   If I need to find an original or known-working version of a file, I **must** retrieve it from the `main` or `master` branch (e.g., via `git show main:<path/to/file>`). I will not ask you for it.

2.  **Establish Baseline:**
    *   I will assume we are operating in a large repository where the primary remote branch (`origin/main` or `origin/master`) is the last known stable state. If uncertain, I will ask.

3.  **Pre-Proposal Diff Review:**
    *   Before proposing changes, I will always review the cumulative diff against the merge-base of the target branch to verify the changes are accurate and safe.
    *   `git diff $(git merge-base origin/main HEAD) HEAD`

4.  **Repository Awareness:**
    *   When asked about the repository's state, I will inspect local Git logs and file diffs to provide informed answers.

5.  **Only Publish Working Code:**
    *   I must never push code to GitHub unless it has been verified to work locally first. This includes running relevant tests, ensuring the application starts without errors, and confirming core functionality works as expected.

6.  **Confirm Before Publishing:**
    *   After successfully committing changes, I will explicitly ask for your confirmation before I push them to the remote GitHub repository.

7.  **Provide Pull Request URL:**
    *   After successfully pushing a new branch with commits, I will provide the direct URL to create a pull request on GitHub.

8.  **Always Link Pull Request URLs:**
    *   When creating or referencing a pull request, I must always provide the direct clickable URL to the PR. This applies to both newly created PRs and when discussing existing ones.

9.  **Default to Main Branch for Pull Requests:**
    *   Unless explicitly instructed otherwise, all pull requests should target the `main` branch as the base. Do not assume `dev` or other branches without confirmation.

10. **Merge Protocol and Branch Management:**
    *   **CRITICAL**: Always use the `integrate` alias pattern for updating from main: `git checkout main && git pull && git branch -D dev && git checkout -b dev`
    *   **Main Branch Protection**: Never work directly on `main` - always use a local `dev` branch for protection
    *   **After Merges**: Always run the integrate pattern to get latest changes and create fresh dev branch
    *   **PR Creation Process**:
        1. Create feature branch from latest main using integrate pattern
        2. Make changes and commit with descriptive messages
        3. **MANDATORY**: Run `./run_tests.sh` and include test results in PR description
        4. Push branch and create PR with comprehensive description using `gh pr create`
        5. Provide clickable PR URL for user review
        6. After merge, immediately run integrate pattern before next PR
        7. **CRITICAL**: Never use 'dev' as a remote branch for PRs - always use descriptive feature branch names
    *   **PR Descriptions**: Always include Summary, Changes, Benefits, and Usage sections
    *   **Commit Messages**: Use descriptive titles with bullet points and Claude Code attribution

11. **GitHub CLI Preference for PR Operations:**
    *   When the user requests to apply/test a pull request, I must prefer using GitHub CLI commands over manual git operations.
    *   **Primary method**: Use `gh pr checkout <PR_NUMBER>` to apply PR changes locally for testing
    *   **Alternative methods**: If GitHub CLI is unavailable, use `git fetch origin pull/<PR_NUMBER>/head:<branch_name>` followed by `git checkout <branch_name>`
    *   This provides a more convenient and reliable workflow for PR testing and reduces the risk of manual errors in branch management.

## IV. Environment, Tooling & Scripts

1.  **Python Virtual Environment Management:**
    *   I will verify that the project-specific virtual environment (`venv`) is activated before running any Python scripts, linters, testers, or package managers. If it's not active, I will attempt to activate it or inform you if I cannot.
2.  **Write Robust & Context-Aware Scripts:**
    *   Automation scripts (e.g., `deploy.sh`) will be designed to be robust, idempotent, and work correctly from any subdirectory.
3.  **Use `vpython` for Tests - Consistent Execution Pattern:**
    *   Always use `vpython` to run tests (e.g., `vpython -m unittest discover` or `vpython path/to/test_file.py`). You have standing permission to run these tests at any time to verify changes, and you do not need to ask for approval first.
    *   **CRITICAL: When user says "run all tests", always use `./run_tests.sh` script from project root instead of manual unittest commands.**
    *   **CRITICAL: When ANY test fails, I must either fix it immediately or explicitly ask the user if it should be fixed. I must highlight the entire line of failing test output in red for visibility.**
    *   **CRITICAL: Consistent Directory Navigation for `vpython`:**
        - If currently in project root: `cd mvp_site && TESTING=true vpython test_file.py`
        - If already in mvp_site: `TESTING=true vpython test_file.py` 
        - If unsure of location: Always use `cd mvp_site && TESTING=true vpython test_file.py`
        - **Never** attempt `cd mvp_site` when already in mvp_site (causes "No such file or directory" error)
    *   **Correct Test Commands:** For this project, use `TESTING=true vpython test_integration.py` from the `mvp_site/` directory for integration tests, or `vpython -m unittest test_module.TestClass.test_method` for specific tests.
4.  **Tool Failure and Recovery Protocol:**
    *   If a command or tool fails more than once, I must stop and try an alternative command or a different approach. I will not repeatedly attempt the same failing action. If a file becomes corrupted or its state is uncertain due to failed edits, my default recovery strategy is to fetch the last known good version from the `main` or `master` branch and restart the editing process.
5.  **Use Full-Content Tools for Web Scraping:**
    *   When the goal is to download the content of a webpage, I must use a tool that retrieves the full page content (e.g., `curl`). I will not use a search tool (like `web_search`) that only returns snippets, as the primary objective is to acquire the complete text.

## V. Knowledge Management & Process Improvement

This protocol uses a set of files in a `.cursor` directory at the project's root to manage our workflow. If they don't exist, I will create them. I will review them before each interaction and update them after.

1.  **.cursor/scratchpad.md - Dynamic Task Management:**
    *   **Purpose:** My active workspace for planning, documenting my thought process, and tracking progress on the current task using checklists.
    *   **Workflow:** I will initialize it for new tasks, break down the task into a step-by-step plan, and update it as I work.

2.  **.cursor/rules/lessons.mdc - Persistent Learnings:**
    *   **Purpose:** A persistent, repository-agnostic knowledge base for reusable techniques, best practices, and insights.
    *   **Workflow:** When we solve a novel problem or I am corrected, I will document the actionable learning here to avoid repeating past mistakes.

3.  **.cursor/project.md - Project-Specific Knowledge Base:**
    *   **Purpose:** A technical knowledge base for *this specific repository*.
    *   **Workflow:** As I work on files, I will document their functionality, APIs, and the "dependency graph" relevant to my tasks to build a focused, evolving design document of the areas I've engaged with.

4.  **"5 Whys" for All Corrections and Failures:**
    *   When a significant error occurs, or whenever you correct a mistake in my process or code, I **must** perform a root cause analysis. The resulting "Actionable Lesson" **must** be documented in `.cursor/rules/lessons.mdc` to prevent that class of error in the future.

5.  **Synchronize with Cursor Settings:**
    *   After we modify this `rules.mdc` file, I will remind you to copy its contents into the "Edit an AI Rule" section of the Cursor settings to ensure my behavior reflects the most current protocol.

6.  **Proactive Rule and Lesson Documentation:**
    *   After completing any significant debugging session, integration test work, or bug fixes, I must proactively update both `.cursor/rules/rules.mdc` and `.cursor/rules/lessons.mdc` with relevant lessons learned, without waiting for explicit instruction.
    *   This ensures knowledge preservation and prevents repeating the same mistakes in future sessions.

## VI. Project-Specific Lessons Log

*This log captures key technical decisions and fixes from our sessions.*

*   **Flask SPA Routing:** A Flask backend serving a SPA must have a catch-all route to serve `index.html` for all non-API paths.
*   **CSS/JS Caching:** To avoid stale static assets during development, restart the dev server and perform a hard refresh in the browser. Cache-busting techniques (e.g., query params) are best for production.
*   **Python `venv` & PEP 668:** To avoid system package conflicts (`externally managed`), always work within a project-specific virtual environment. On some systems, `python3-venv` may need to be installed via the system package manager first.
*   **Shell Config (`.bashrc`):** Changes to shell configs require sourcing the file (e.g., `source ~/.bashrc`) or starting a new session to take effect.
*   **LLM System Prompts:** Detailed, explicit, and well-structured system prompts are crucial for improving AI performance and consistency.
*   **Dotfile Backups:** Critical configuration files in transient environments (like Cloud Shell) should be version-controlled or backed up.

- After implementing a feature or a fix, I should always offer to run the relevant unit tests to verify the changes.

- **Before attempting a complex solution, such as refactoring code or changing core logic, first explicitly state and evaluate the simplest possible solution that could achieve the user's goal.** This forces a "simple-first" approach, prioritizing the most direct path to the objective over unnecessary refactoring.

## VII. Data Integrity and AI Management

1.  **Prioritize Data Integrity:** When handling data from any source (database, API, AI), I must assume the data may be incomplete or malformed. I will defensively access all dictionary keys and object properties (e.g., using `dict.get()` or optional chaining) and validate data structures before processing them.
2.  **Enforce Critical Logic in Code:** For operations that are critical to data integrity (like appending to a log or preventing state corruption), I will always implement safeguards and validation in the application code rather than relying solely on instructing an AI through prompts.
3.  **Verify Data Paths:** When investigating bugs related to data not being saved, my first step will be to verify and log the full read path and the full write path to ensure they are identical.
4.  **Maintain a Single Source of Truth for AI Instructions:** When creating or modifying instructional documents for an AI (e.g., prompt files), I must ensure there is one, and only one, clear and unambiguous way to perform a given task. I will remove or refactor any conflicting examples or rules.
5.  **Always Check for File Existence Before Creating:** Before writing to a file that I believe might be new (like a configuration or documentation file), I must first use a command like `ls` to verify whether it already exists. If it does, I must read it and append to it, rather than overwriting it.

## VIII. User Communication & Formatting

1.  **Raw Markdown for "Markdown Format":** When you ask for output in "markdown format" (e.g., for a pull request description), I will provide the raw, unrendered Markdown text enclosed in a ` ```markdown ... ``` ` code block. This ensures the content is easy for you to copy and paste directly.

## IX. Code Review and Integration Validation

### Critical Review Process
When reviewing code changes or merges, ALWAYS follow this validation process:

1. **Validate AI Instructions Have Implementation**: Always verify that documented AI capabilities (especially in `prompts/` directory) have corresponding code implementation
2. **Trace Data Flow End-to-End**: For any AI-driven feature, trace the complete pipeline: AI instruction → parsing → state updates → storage
3. **Integration Testing Over Syntax**: Treat reviews as functional integration tests, not just code quality checks
4. **Search for Missing Implementations**: Before approving merges, search codebase for referenced tokens/features (e.g., `__DELETE__`, special syntax, documented functions)

### Common Pitfalls to Avoid
- **Never assume documentation equals working implementation** - Always verify
- Focusing on code structure without validating complete data pipelines
- Treating AI instructions as truth rather than specifications to verify
- Missing gaps between what AI is instructed to do vs what code actually processes

### Validation Commands
```bash
# Search for documented features in prompts
grep -r "__DELETE__" prompts/
grep -r "special_token" prompts/

# Verify implementation exists
grep -r "__DELETE__" *.py
grep -r "process.*special_token" *.py
```

**Critical Lesson**: A bug was missed where AI was instructed to use `__DELETE__` tokens for defeated enemies, but no code existed to process these tokens, causing combat state inconsistencies. This type of documentation-implementation gap must be caught during review.

## X. Critical Lessons and Rules

### Automatic Rule Updates
**MANDATORY**: Whenever I make a mistake, encounter a bug I should have caught, or receive correction from the user, I MUST immediately update both this rules.mdc file and .cursor/rules/lessons.mdc with the lesson learned. I will not wait for the user to remind me - this is an automatic responsibility that happens every time I fail or am corrected.

### Data Corruption Pattern Analysis
**CRITICAL RULE**: When encountering ANY data corruption bug, treat it as a systemic issue requiring comprehensive pattern analysis:
- Search for ALL similar corruption patterns across the codebase (e.g., `str()` conversions, type changes)
- Identify ALL code paths that process the same data structures
- Apply the principle: "If there's one bug of this type, there are likely others"
- Create data integrity audit checklists for similar data structures

**Lesson**: Missed NPC data corruption because I focused on isolated `__DELETE__` bug without auditing all `str()` conversions that could corrupt structured data.

### AI Instruction Priority and Ordering
**CRITICAL RULE**: When AI systems have multiple competing instructions, instruction ORDER determines compliance:
- **Most critical instructions MUST be loaded first** (e.g., state management, core protocols)
- Later instructions can override or distract from earlier ones
- Long instruction sets suffer from "instruction fatigue" where later rules are ignored
- Always prioritize core functionality over stylistic preferences in prompt ordering

**Lesson**: AI was ignoring state update requirements because game state instructions were loaded LAST after lengthy narrative instructions. Moving them FIRST fixed the core state update failure.

### Data Structure Schema Enforcement 
**CRITICAL RULE**: When AI systems generate structured data, explicit schemas with mandatory data types must be enforced through prompt engineering:
- **Schema Definition**: Clearly define expected data structures (lists vs dictionaries, required fields)
- **Contradictory Instructions**: Remove any conflicting examples or deprecated patterns that confuse the AI
- **Type Validation**: Add explicit rules forbidding type changes (e.g., "active_missions MUST be a list, never a dictionary")
- **Example Clarity**: Provide concrete examples of correct vs incorrect formatting

**Lesson**: Fixed critical data corruption where AI was changing `active_missions` from list to dictionary and using incorrect `combat_state` schemas by adding explicit data type rules and removing contradictory dot notation examples.

### Codebase Exploration Protocol
**CRITICAL RULE**: When working with any codebase, ALWAYS follow this sequence:
1. **Run tests FIRST** - before reading any code, run the full test suite to establish baseline understanding of what works/breaks
2. **Read project documentation** - Review CLAUDE.md, README, and project instructions
3. **Investigate specific issues** - Only then dive into individual files and components

**Lesson**: Failed to catch multiple dependency and API inconsistency bugs because I explored code files individually instead of running tests first. The test suite immediately reveals: current state, expected behavior, dependency issues, and API changes. This systematic approach prevents iterative discovery of issues that should be obvious upfront.

## XI. Deployment

- Multi-environment deployment via `./deploy.sh` script
- Docker containerization with health checks and 300s timeout
- Google Cloud Run with secret management for API keys
- Automatic context-aware deployment from any directory

