---
description: /generatetest - Evidence-Based Test Generator (Real Mode Only)
type: llm-orchestration
execution_mode: immediate
---
## ‚ö° EXECUTION INSTRUCTIONS FOR CLAUDE
**When this command is invoked, YOU (Claude) must execute these steps immediately.**
**This is NOT documentation - these are COMMANDS to execute right now.**

## üö® CORE PRINCIPLES

**REAL MODE ONLY**: All generated tests use real local servers, real databases, nothing mocked.
**EVIDENCE-FIRST**: Tests generate savetmp-compatible evidence bundles to `/tmp/<repo>/<branch>/<work>/<timestamp>/`.
**FREE-FORM INPUT**: Accept natural language like "for this PR make sure the equipment logic works".

## üìÅ OUTPUT LOCATIONS

| Output Type | Default Location | Override Flag |
|-------------|------------------|---------------|
| **Test files** | `testing_mcp/` | `--test-dir <path>` |
| **Evidence** | `/tmp/<repo>/<branch>/<work>/<timestamp>/` | `--evidence-dir <path>` |

## üö® EXECUTION WORKFLOW

### Phase 1: Parse Free-Form Input

**Action Steps:**
1. Extract test focus from natural language input (e.g., "equipment logic", "dice rolls", "campaign creation")
2. Identify PR context if mentioned (e.g., "for this PR" ‚Üí analyze current branch changes)
3. Determine test type: MCP integration, browser automation, or hybrid
4. Generate descriptive `work_name` for evidence directory

**Example Parsing:**
```text
Input: "for this PR make sure the equipment logic works"
‚Üí Focus: equipment logic
‚Üí Context: current PR/branch changes
‚Üí Type: MCP integration (equipment = game state)
‚Üí work_name: equipment_validation
```

### Phase 2: Generate Test File

**Action Steps:**
1. Create test file in `testing_mcp/test_<focus>.py` (or custom `--test-dir`)
2. Include savetmp-compatible evidence generation (methodology, evidence, notes)
3. Add `--savetmp` and `--work-name` CLI arguments to test
4. Ensure test uses REAL servers (no mocks, no test mode)

**Generated Test Structure:**
```python
#!/usr/bin/env python3
"""
Generated by /generatetest - Evidence-Based Test
Focus: [extracted focus]
Work Name: [work_name]

REAL MODE ONLY - No mocks, no test mode
"""
import argparse
import json
import os
import subprocess
from datetime import datetime, timezone
from pathlib import Path

# Test configuration
SERVER_URL = "http://localhost:8082"  # Real local server
WORK_NAME = "[work_name]"

def generate_savetmp_docs(results, git_info, server_url):
    """Generate methodology/evidence/notes from actual test data."""
    # Methodology: derive from actual environment, not hardcoded
    dev_mode = os.environ.get("WORLDAI_DEV_MODE", "not set")
    methodology = f"""# Test Methodology
## Environment
- Server: {server_url}
- WORLDAI_DEV_MODE: {dev_mode}
- Timestamp: {datetime.now(timezone.utc).isoformat()}

## Test Scope
[Generated from actual test execution]
"""

    # Evidence: derive from actual results
    passed = sum(1 for r in results if r.passed)
    total = len(results)
    evidence = f"""# Evidence Summary
## Results: {passed}/{total} PASS

| Test | Status | Details |
|------|--------|---------|
"""
    for r in results:
        status = "‚úÖ PASS" if r.passed else "‚ùå FAIL"
        evidence += f"| {r.name} | {status} | {r.details} |\n"

    # Notes: warnings and follow-ups
    notes = "# Notes\n"
    # Add any warnings from test execution

    return methodology, evidence, notes

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--server", default=SERVER_URL)
    parser.add_argument("--savetmp", action="store_true",
                        help="Save evidence to /tmp structure")
    parser.add_argument("--work-name", default=WORK_NAME)
    args = parser.parse_args()

    # Run tests against REAL server
    results = run_tests(args.server)

    if args.savetmp:
        # Generate savetmp-compatible evidence
        git_info = capture_git_provenance()
        methodology, evidence, notes = generate_savetmp_docs(
            results, git_info, args.server
        )

        # Call savetmp.py with generated docs
        subprocess.run([
            "python", ".claude/commands/savetmp.py", args.work_name,
            "--methodology", methodology,
            "--evidence", evidence,
            "--notes", notes
        ], check=True)

if __name__ == "__main__":
    main()
```

### Phase 3: Add Evidence Standards Compliance

**Action Steps:**
1. Include git provenance capture (HEAD, origin/main, changed files)
2. Add server environment capture (process info, ports, env vars)
3. Derive ALL documentation from actual data (never hardcode)
4. Track missing/dropped data with warnings
5. Check subprocess return codes

**Evidence Standards Checklist (from `.claude/skills/evidence-standards.md`):**
- [ ] Git provenance: HEAD commit, origin/main, changed files
- [ ] Server environment: PID, port, WORLDAI_DEV_MODE
- [ ] Checksums: SHA256 for all evidence files
- [ ] Timestamp synchronization: collect all evidence in one pass
- [ ] Documentation-Data alignment: derive claims from actual data

### Phase 4: Verify Real Mode

**Action Steps:**
1. Confirm server is running on expected port
2. Verify WORLDAI_DEV_MODE setting
3. Ensure NO mock imports or test mode flags
4. Validate API responses are from real server

**üö® MOCK MODE = INVALID EVIDENCE**:
- ‚ùå FORBIDDEN: `TESTING=true`, mock imports, fake services
- ‚ùå FORBIDDEN: Hardcoded responses or placeholder data
- ‚úÖ REQUIRED: Real local server, real database, real API responses

## üìã REFERENCE DOCUMENTATION

# /generatetest - Evidence-Based Test Generator

**Purpose**: Generate tests with built-in savetmp-compatible evidence generation

**Usage**: `/generatetest <free-form description>`

**Examples:**
```bash
/generatetest for this PR make sure the equipment logic works
/generatetest validate dice roll integrity in combat
/generatetest test campaign creation flow end-to-end
```

## üîç TEST TYPE DETECTION

**Automatic Detection from Free-Form Input:**

| Keywords | Test Type | Example Input |
|----------|-----------|---------------|
| `equipment`, `inventory`, `items`, `game_state` | MCP Integration | "equipment logic works" |
| `dice`, `roll`, `combat`, `damage` | MCP Integration | "dice rolls are fair" |
| `campaign`, `create`, `firebase` | MCP Integration | "campaign creation flow" |
| `browser`, `ui`, `page`, `click` | Browser Automation | "landing page loads correctly" |
| `login`, `oauth`, `auth` | Browser + Auth | "login flow works" |

**Default**: MCP Integration (most common for this project)

## üìä GENERATED TEST REQUIREMENTS

Every generated test MUST include:

### 1. Evidence Generation Function
```python
def generate_savetmp_docs(results, git_info, server_url, run_dir):
    """Generate methodology/evidence/notes from ACTUAL test data."""
    # ‚úÖ Derive from os.environ, not hardcoded
    dev_mode = os.environ.get("WORLDAI_DEV_MODE", "not set")

    # ‚úÖ Track missing data with warnings
    missing_items = []

    # ‚úÖ Use correct denominators (found/total, not found/min)
    stats_col = f"{found}/{len(total_items)} (need {min_required})"
```

### 2. CLI Arguments for Evidence
```python
parser.add_argument("--savetmp", action="store_true")
parser.add_argument("--work-name", default="<auto_generated>")
```

### 3. Real Mode Verification
```python
def verify_real_mode(server_url):
    """Verify server is real, not mocked."""
    response = requests.get(f"{server_url}/health")
    assert response.status_code == 200
    assert "mock" not in response.text.lower()
```

### 4. Git Provenance Capture
```python
def capture_git_provenance():
    """Capture git state for evidence."""
    return {
        "head_commit": subprocess.check_output(["git", "rev-parse", "HEAD"]).decode().strip(),
        "branch": subprocess.check_output(["git", "branch", "--show-current"]).decode().strip(),
        "origin_main": subprocess.check_output(["git", "rev-parse", "origin/main"]).decode().strip(),
        "changed_files": subprocess.check_output(
            ["git", "diff", "--name-only", "origin/main...HEAD"]
        ).decode().strip().split("\n"),
    }
```

## üö® EVIDENCE STANDARDS COMPLIANCE

From `.claude/skills/evidence-standards.md`:

| Requirement | Implementation |
|-------------|----------------|
| **Derive claims from data** | `os.environ.get()`, not hardcoded strings |
| **Warn on missing data** | Track `missing_item_ids` list, add to notes |
| **Correct denominators** | `found/total (need min)`, not `found/min` |
| **Check return codes** | `if result.returncode != 0: warn()` |
| **Single run attribution** | Evidence bundle references ONE test run |
| **Git provenance** | HEAD, origin/main, changed files |
| **Checksums** | SHA256 via savetmp.py |

## üîß PRIORITY MATRIX

```text
üö® CRITICAL: Blocks core functionality, data corruption risk
‚ö†Ô∏è HIGH: Significant degradation, wrong behavior
üìù MEDIUM: Minor issues, cosmetic problems
‚ÑπÔ∏è LOW: Documentation, edge cases
```

**Stop Rule**: üö® CRITICAL ‚Üí Stop testing, fix immediately, verify, resume

## ‚úÖ COMPLETION CRITERIA

- [ ] Test file created in `testing_mcp/` (or custom dir)
- [ ] Evidence generation integrated (methodology, evidence, notes)
- [ ] Real mode verified (no mocks, no TESTING=true)
- [ ] Git provenance captured
- [ ] All results derived from actual data
- [ ] Missing data tracked with warnings
