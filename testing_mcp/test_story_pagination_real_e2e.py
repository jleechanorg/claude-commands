#!/usr/bin/env python3
"""Story Pagination E2E tests against real MCP server with real LLM API.

Generated by /generatetest - Evidence-Based Test
Focus: Story pagination API endpoint
Work Name: story_pagination

REAL MODE ONLY - No mocks, no test mode

This test validates the story-pagination feature (PR #3144):
- GET /api/campaigns/<id>/story endpoint with pagination
- pagination metadata (has_older, total_count, oldest_timestamp, oldest_id)
- Cursor-based pagination with before_timestamp/before_id
- Chronological ordering within pages
- No duplicate/missing entries between pages

Run (local MCP already running):
    cd testing_mcp
    python test_story_pagination_real_e2e.py --server-url http://127.0.0.1:8001

Run (start local MCP automatically; requires `gcloud` access to Secret Manager):
    cd testing_mcp
    python test_story_pagination_real_e2e.py --start-local

Run with evidence generation:
    cd testing_mcp
    python test_story_pagination_real_e2e.py --start-local --savetmp
"""

# ruff: noqa: PLR0912, PLR0915, DTZ005

from __future__ import annotations

import argparse
import json
import os
import subprocess
import sys
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

import requests

sys.path.insert(0, str(Path(__file__).parent))

from lib.campaign_utils import create_campaign, process_action
from lib.evidence_utils import (
    capture_provenance,
    create_evidence_bundle,
    get_evidence_dir,
    write_with_checksum,
)
from lib.server_utils import (
    DEFAULT_EVIDENCE_ENV,
    LocalServer,
    PROJECT_ROOT,
    pick_free_port,
)
from lib.mcp_client import MCPClient

# Constants
WORK_NAME = "story_pagination"
DEFAULT_SERVER_URL = "http://127.0.0.1:8001"
MIN_STORY_ENTRIES = 6  # Minimum entries needed to test pagination

# Test auth bypass headers (only work when server started with TESTING=true)
# NOTE: TESTING=true only enables auth bypass, NOT mock mode
# MOCK_SERVICES_MODE=false ensures real LLM and Firestore
TEST_AUTH_HEADERS = {
    "X-Test-Bypass-Auth": "true",
    "X-Test-User-ID": "",  # Will be set per request
}


def start_flask_server(
    port: int,
    *,
    env_overrides: dict[str, str] | None = None,
) -> LocalServer:
    """Start Flask main.py server with TESTING=true for auth bypass.

    Uses REAL services:
    - MOCK_SERVICES_MODE=false (real Firestore, real LLM)
    - TESTING=true (enables auth bypass headers only)
    """
    python_bin = PROJECT_ROOT / "venv" / "bin" / "python"
    if not python_bin.exists():
        python_bin = Path(sys.executable)

    env = dict(os.environ)
    env["PYTHONPATH"] = str(PROJECT_ROOT)

    # Apply default evidence capture settings
    for key, value in DEFAULT_EVIDENCE_ENV.items():
        env.setdefault(key, value)

    if env_overrides:
        env.update(env_overrides)

    # CRITICAL: Real services, auth bypass only
    env["MOCK_SERVICES_MODE"] = "false"  # Real Firestore + LLM
    env["TESTING"] = "true"  # Auth bypass headers enabled

    # Dev mode acknowledgement for real Firestore
    if env.get("WORLDAI_GOOGLE_APPLICATION_CREDENTIALS") and not env.get("WORLDAI_DEV_MODE"):
        env["WORLDAI_DEV_MODE"] = "true"

    # Set port via environment variable (Flask uses PORT env var)
    env["PORT"] = str(port)

    log_root = Path("/tmp/mcp_server_logs")
    log_root.mkdir(parents=True, exist_ok=True)
    log_path = log_root / f"flask_main_{port}.log"
    log_f = open(log_path, "wb")  # noqa: SIM115

    proc = subprocess.Popen(
        [
            str(python_bin),
            "-m",
            "mvp_site.main",
        ],
        cwd=str(PROJECT_ROOT),
        env=env,
        stdout=log_f,
        stderr=subprocess.STDOUT,
    )

    return LocalServer(
        proc=proc,
        base_url=f"http://127.0.0.1:{port}",
        log_path=log_path,
        _log_file=log_f,
    )


# Player actions to build up story - designed for variety and quick responses
PLAYER_ACTIONS = [
    "I look around the tavern and observe who else is here.",
    "I approach the bartender and order an ale.",
    "I ask the bartender if he's heard any rumors about nearby dungeons.",
    "I walk over to the notice board and read the posted quests.",
    "I check my equipment and make sure my sword is sharp.",
    "I head outside and survey the town square.",
    "I visit the local blacksmith to browse weapons.",
    "I return to the tavern and find a quiet corner to rest.",
]


@dataclass
class TestResult:
    """Result of a single test."""

    name: str
    passed: bool
    details: str
    data: dict[str, Any] = field(default_factory=dict)


@dataclass
class PaginationCapture:
    """Captured pagination API response for evidence."""

    endpoint: str
    params: dict[str, str]
    response: dict[str, Any]
    timestamp: str
    entry_ids: list[str]
    entry_timestamps: list[str]


@dataclass
class PaginationTestContext:
    """Context for pagination tests."""

    client: MCPClient
    user_id: str
    campaign_id: str
    all_entry_ids: list[str] = field(default_factory=list)
    all_timestamps: list[str] = field(default_factory=list)
    pagination_captures: list[PaginationCapture] = field(default_factory=list)


def verify_real_mode(server_url: str) -> bool:
    """Verify server is real, not mocked."""
    try:
        # Try health endpoint
        response = requests.get(f"{server_url}/health", timeout=5)
        if response.status_code == 200:
            return True
        # Try MCP endpoint
        response = requests.get(f"{server_url}/mcp", timeout=5)
        return response.status_code in (200, 405)  # 405 = method not allowed but endpoint exists
    except requests.RequestException:
        return False


def wait_for_server(server_url: str, timeout: int = 60) -> bool:
    """Wait for server to become available."""
    start = time.time()
    while time.time() - start < timeout:
        if verify_real_mode(server_url):
            return True
        time.sleep(1)
    return False


def fetch_story_page(
    server_url: str,
    user_id: str,
    campaign_id: str,
    limit: int = 100,
    before: str | None = None,
    before_id: str | None = None,
    *,
    capture_list: list[PaginationCapture] | None = None,
) -> dict[str, Any]:
    """Fetch a page of story entries via the REST API with test auth bypass.

    Args:
        server_url: Base URL of the server.
        user_id: User ID for auth bypass header.
        campaign_id: Campaign to fetch story from.
        limit: Max entries per page.
        before: Cursor timestamp for pagination.
        before_id: Cursor entry ID for pagination.
        capture_list: Optional list to capture responses for evidence.

    Returns:
        API response dict with 'story' and 'pagination' keys.
    """
    params: dict[str, str] = {"limit": str(limit)}
    if before:
        params["before"] = before
    if before_id:
        params["before_id"] = before_id

    # Use test auth bypass headers (requires TESTING=true on server)
    url = f"{server_url}/api/campaigns/{campaign_id}/story"
    headers = {
        "X-Test-Bypass-Auth": "true",
        "X-Test-User-ID": user_id,
    }

    response = requests.get(url, params=params, headers=headers, timeout=30)
    response.raise_for_status()
    result = response.json()

    # Capture for evidence if requested
    if capture_list is not None:
        story = result.get("story", [])
        capture = PaginationCapture(
            endpoint=f"/api/campaigns/{campaign_id}/story",
            params=params,
            response=result,
            timestamp=datetime.now(timezone.utc).isoformat(),
            entry_ids=[e.get("id", "") for e in story if e.get("id")],
            entry_timestamps=[e.get("timestamp", "") for e in story if e.get("timestamp")],
        )
        capture_list.append(capture)

    return result


def test_campaign_creation(ctx: PaginationTestContext) -> TestResult:
    """Test 1: Create campaign successfully."""
    try:
        # Campaign already created in context
        if ctx.campaign_id:
            return TestResult(
                name="Campaign Creation",
                passed=True,
                details=f"Campaign created: {ctx.campaign_id}",
                data={"campaign_id": ctx.campaign_id},
            )
        return TestResult(
            name="Campaign Creation",
            passed=False,
            details="No campaign_id in context",
        )
    except Exception as e:
        return TestResult(
            name="Campaign Creation",
            passed=False,
            details=f"Error: {e}",
        )


def test_build_story_entries(ctx: PaginationTestContext, server_url: str) -> TestResult:
    """Test 2: Build up story entries with real LLM actions."""
    entries_created = 0
    errors: list[str] = []

    for i, action in enumerate(PLAYER_ACTIONS):
        try:
            print(f"  Processing action {i + 1}/{len(PLAYER_ACTIONS)}: {action[:50]}...")
            result = process_action(
                ctx.client,
                user_id=ctx.user_id,
                campaign_id=ctx.campaign_id,
                user_input=action,
                mode="character",
            )

            if result.get("error"):
                errors.append(f"Action {i + 1}: {result['error']}")
            else:
                entries_created += 1
                # Small delay to ensure distinct timestamps
                time.sleep(0.5)

        except Exception as e:
            errors.append(f"Action {i + 1}: {e}")

    passed = entries_created >= MIN_STORY_ENTRIES
    details = f"Created {entries_created}/{len(PLAYER_ACTIONS)} entries"
    if errors:
        details += f"; Errors: {len(errors)}"

    return TestResult(
        name="Build Story Entries",
        passed=passed,
        details=details,
        data={"entries_created": entries_created, "errors": errors},
    )


def test_pagination_metadata(ctx: PaginationTestContext, server_url: str) -> TestResult:
    """Test 3: Verify pagination metadata is present and valid."""
    try:
        response = fetch_story_page(
            server_url, ctx.user_id, ctx.campaign_id, limit=3,
            capture_list=ctx.pagination_captures,
        )

        # Check required fields
        required_fields = ["story", "pagination"]
        missing_fields = [f for f in required_fields if f not in response]

        if missing_fields:
            return TestResult(
                name="Pagination Metadata",
                passed=False,
                details=f"Missing fields: {missing_fields}",
                data={"response_keys": list(response.keys())},
            )

        pagination = response.get("pagination", {})
        pagination_fields = ["has_older", "total_count", "oldest_timestamp"]
        missing_pagination = [f for f in pagination_fields if f not in pagination]

        if missing_pagination:
            return TestResult(
                name="Pagination Metadata",
                passed=False,
                details=f"Missing pagination fields: {missing_pagination}",
                data={"pagination": pagination},
            )

        # Store for later tests
        story = response.get("story", [])
        for entry in story:
            if entry.get("id"):
                ctx.all_entry_ids.append(entry["id"])
            if entry.get("timestamp"):
                ctx.all_timestamps.append(entry["timestamp"])

        return TestResult(
            name="Pagination Metadata",
            passed=True,
            details=f"total_count={pagination.get('total_count')}, has_older={pagination.get('has_older')}",
            data={"pagination": pagination, "story_count": len(story)},
        )

    except Exception as e:
        return TestResult(
            name="Pagination Metadata",
            passed=False,
            details=f"Error: {e}",
        )


def test_limit_enforcement(ctx: PaginationTestContext, server_url: str) -> TestResult:
    """Test 4: Verify small limits are respected by the API."""
    try:
        results: list[tuple[int, int, int]] = []
        for limit in (2, 3):
            response = fetch_story_page(
                server_url,
                ctx.user_id,
                ctx.campaign_id,
                limit=limit,
                capture_list=ctx.pagination_captures,
            )
            story = response.get("story", [])
            pagination = response.get("pagination", {})
            fetched = pagination.get("fetched_count", len(story))
            results.append((limit, len(story), fetched))

            if len(story) > limit or fetched > limit:
                return TestResult(
                    name="Limit Enforcement",
                    passed=False,
                    details=(
                        f"limit={limit}: story_count={len(story)}, "
                        f"fetched_count={fetched}"
                    ),
                    data={
                        "limit": limit,
                        "story_count": len(story),
                        "fetched_count": fetched,
                    },
                )

        details = ", ".join(
            f"limit={limit}: story_count={story_count}, fetched_count={fetched_count}"
            for limit, story_count, fetched_count in results
        )
        return TestResult(
            name="Limit Enforcement",
            passed=True,
            details=details,
        )

    except Exception as e:
        return TestResult(
            name="Limit Enforcement",
            passed=False,
            details=f"Error: {e}",
        )


def test_has_older_flag(ctx: PaginationTestContext, server_url: str) -> TestResult:
    """Test 5: Verify has_older flag is correct."""
    try:
        # Fetch with small limit - should have has_older=True if enough entries
        response_small = fetch_story_page(
            server_url, ctx.user_id, ctx.campaign_id, limit=2,
            capture_list=ctx.pagination_captures,
        )
        pagination_small = response_small.get("pagination", {})
        total = pagination_small.get("total_count", 0)
        has_older_small = pagination_small.get("has_older", False)

        # Fetch all entries - should have has_older=False
        response_all = fetch_story_page(
            server_url, ctx.user_id, ctx.campaign_id, limit=1000,
            capture_list=ctx.pagination_captures,
        )
        pagination_all = response_all.get("pagination", {})
        has_older_all = pagination_all.get("has_older", False)

        # Validate
        expected_has_older_small = total > 2
        passed = (has_older_small == expected_has_older_small) and (has_older_all is False)

        details = (
            f"limit=2: has_older={has_older_small} (expected={expected_has_older_small}); "
            f"limit=1000: has_older={has_older_all} (expected=False)"
        )

        return TestResult(
            name="has_older Flag",
            passed=passed,
            details=details,
            data={
                "total": total,
                "has_older_small": has_older_small,
                "has_older_all": has_older_all,
            },
        )

    except Exception as e:
        return TestResult(
            name="has_older Flag",
            passed=False,
            details=f"Error: {e}",
        )


def test_cursor_pagination(ctx: PaginationTestContext, server_url: str) -> TestResult:
    """Test 6: Verify cursor-based pagination with before_timestamp."""
    try:
        # Get first page
        page1 = fetch_story_page(
            server_url, ctx.user_id, ctx.campaign_id, limit=3,
            capture_list=ctx.pagination_captures,
        )
        pagination1 = page1.get("pagination", {})
        story1 = page1.get("story", [])

        if not pagination1.get("has_older"):
            return TestResult(
                name="Cursor Pagination",
                passed=True,
                details="Not enough entries to test pagination (has_older=False)",
                data={"entries": len(story1)},
            )

        # Get second page using cursor
        oldest_ts = pagination1.get("oldest_timestamp")
        oldest_id = pagination1.get("oldest_id")

        if not oldest_ts:
            return TestResult(
                name="Cursor Pagination",
                passed=False,
                details="No oldest_timestamp in pagination response",
            )

        page2 = fetch_story_page(
            server_url,
            ctx.user_id,
            ctx.campaign_id,
            limit=3,
            before=oldest_ts,
            before_id=oldest_id,
            capture_list=ctx.pagination_captures,
        )
        story2 = page2.get("story", [])

        # Verify no overlap
        ids1 = {e.get("id") for e in story1 if e.get("id")}
        ids2 = {e.get("id") for e in story2 if e.get("id")}
        overlap = ids1 & ids2

        if overlap:
            return TestResult(
                name="Cursor Pagination",
                passed=False,
                details=f"Duplicate entries between pages: {overlap}",
                data={"overlap": list(overlap)},
            )

        # Verify page2 entries are older than page1
        if story1 and story2:
            newest_page2_ts = max(e.get("timestamp", "") for e in story2)
            oldest_page1_ts = min(e.get("timestamp", "") for e in story1)

            if newest_page2_ts >= oldest_page1_ts:
                return TestResult(
                    name="Cursor Pagination",
                    passed=False,
                    details=f"Page 2 entries not older: {newest_page2_ts} >= {oldest_page1_ts}",
                )

        return TestResult(
            name="Cursor Pagination",
            passed=True,
            details=f"Page 1: {len(story1)} entries, Page 2: {len(story2)} entries, no overlap",
            data={"page1_count": len(story1), "page2_count": len(story2)},
        )

    except Exception as e:
        return TestResult(
            name="Cursor Pagination",
            passed=False,
            details=f"Error: {e}",
        )


def test_chronological_ordering(ctx: PaginationTestContext, server_url: str) -> TestResult:
    """Test 7: Verify entries are in chronological order within each page."""
    try:
        response = fetch_story_page(
            server_url, ctx.user_id, ctx.campaign_id, limit=100,
            capture_list=ctx.pagination_captures,
        )
        story = response.get("story", [])

        if len(story) < 2:
            return TestResult(
                name="Chronological Ordering",
                passed=True,
                details="Not enough entries to verify ordering",
            )

        timestamps = [e.get("timestamp", "") for e in story if e.get("timestamp")]

        # Verify ascending order (oldest to newest)
        is_sorted = timestamps == sorted(timestamps)

        if not is_sorted:
            # Find first out-of-order pair
            for i in range(len(timestamps) - 1):
                if timestamps[i] > timestamps[i + 1]:
                    return TestResult(
                        name="Chronological Ordering",
                        passed=False,
                        details=f"Out of order at index {i}: {timestamps[i]} > {timestamps[i+1]}",
                    )

        return TestResult(
            name="Chronological Ordering",
            passed=True,
            details=f"All {len(timestamps)} entries in chronological order",
            data={"entry_count": len(timestamps)},
        )

    except Exception as e:
        return TestResult(
            name="Chronological Ordering",
            passed=False,
            details=f"Error: {e}",
        )


def test_no_missing_entries(ctx: PaginationTestContext, server_url: str) -> TestResult:
    """Test 8: Verify all entries are accessible via pagination (no gaps)."""
    try:
        # Get total count
        response_first = fetch_story_page(
            server_url, ctx.user_id, ctx.campaign_id, limit=2,
            capture_list=ctx.pagination_captures,
        )
        total_count = response_first.get("pagination", {}).get("total_count", 0)

        # Collect all entries via pagination
        all_entries: list[dict[str, Any]] = []
        before_ts: str | None = None
        before_id: str | None = None
        pages = 0
        max_pages = 20  # Safety limit

        while pages < max_pages:
            response = fetch_story_page(
                server_url,
                ctx.user_id,
                ctx.campaign_id,
                limit=3,
                before=before_ts,
                before_id=before_id,
                capture_list=ctx.pagination_captures,
            )
            story = response.get("story", [])
            pagination = response.get("pagination", {})

            all_entries.extend(story)
            pages += 1

            if not pagination.get("has_older") or not story:
                break

            before_ts = pagination.get("oldest_timestamp")
            before_id = pagination.get("oldest_id")

        # Verify we got all entries
        unique_ids = {e.get("id") for e in all_entries if e.get("id")}

        if len(unique_ids) != total_count:
            return TestResult(
                name="No Missing Entries",
                passed=False,
                details=f"Got {len(unique_ids)} unique entries, expected {total_count}",
                data={"found": len(unique_ids), "expected": total_count},
            )

        return TestResult(
            name="No Missing Entries",
            passed=True,
            details=f"All {total_count} entries accessible via {pages} pages",
            data={"total": total_count, "pages": pages},
        )

    except Exception as e:
        return TestResult(
            name="No Missing Entries",
            passed=False,
            details=f"Error: {e}",
        )


def run_tests(
    server_url: str,
    user_id: str = "test_pagination_user",
) -> tuple[list[TestResult], PaginationTestContext]:
    """Run all pagination tests."""
    results: list[TestResult] = []

    # Initialize client
    client = MCPClient(server_url)

    # Create campaign
    print("\n[1/8] Creating campaign...")
    try:
        campaign_id = create_campaign(
            client,
            user_id,
            title="Pagination Test Campaign",
            character="Kira the Ranger (DEX 16, WIS 14)",
            setting="A bustling frontier town with a popular tavern",
            description="Test campaign for story pagination validation",
        )
        print(f"      Campaign ID: {campaign_id}")
    except Exception as e:
        print(f"      FAILED: {e}")
        results.append(TestResult("Campaign Creation", False, str(e)))
        return results, PaginationTestContext(client, user_id, "")

    ctx = PaginationTestContext(client=client, user_id=user_id, campaign_id=campaign_id)
    results.append(test_campaign_creation(ctx))

    # Build story entries
    print("\n[2/8] Building story entries with real LLM...")
    result = test_build_story_entries(ctx, server_url)
    results.append(result)
    print(f"      {result.details}")

    if not result.passed:
        print("      WARNING: Not enough entries created, pagination tests may be limited")

    # Test pagination metadata
    print("\n[3/8] Testing pagination metadata...")
    result = test_pagination_metadata(ctx, server_url)
    results.append(result)
    print(f"      {'PASS' if result.passed else 'FAIL'}: {result.details}")

    # Test limit enforcement
    print("\n[4/8] Testing limit enforcement...")
    result = test_limit_enforcement(ctx, server_url)
    results.append(result)
    print(f"      {'PASS' if result.passed else 'FAIL'}: {result.details}")

    # Test has_older flag
    print("\n[5/8] Testing has_older flag...")
    result = test_has_older_flag(ctx, server_url)
    results.append(result)
    print(f"      {'PASS' if result.passed else 'FAIL'}: {result.details}")

    # Test cursor pagination
    print("\n[6/8] Testing cursor pagination...")
    result = test_cursor_pagination(ctx, server_url)
    results.append(result)
    print(f"      {'PASS' if result.passed else 'FAIL'}: {result.details}")

    # Test chronological ordering
    print("\n[7/8] Testing chronological ordering...")
    result = test_chronological_ordering(ctx, server_url)
    results.append(result)
    print(f"      {'PASS' if result.passed else 'FAIL'}: {result.details}")

    # Test no missing entries
    print("\n[8/8] Testing no missing entries...")
    result = test_no_missing_entries(ctx, server_url)
    results.append(result)
    print(f"      {'PASS' if result.passed else 'FAIL'}: {result.details}")

    return results, ctx


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Story Pagination E2E tests with real LLM API"
    )
    parser.add_argument(
        "--server-url",
        default=DEFAULT_SERVER_URL,
        help=f"MCP server URL (default: {DEFAULT_SERVER_URL})",
    )
    parser.add_argument(
        "--start-local",
        action="store_true",
        help="Start a local MCP server automatically",
    )
    parser.add_argument(
        "--savetmp",
        action="store_true",
        help="Save evidence to savetmp-compatible structure",
    )
    parser.add_argument(
        "--work-name",
        default=WORK_NAME,
        help=f"Work name for evidence directory (default: {WORK_NAME})",
    )
    parser.add_argument(
        "--user-id",
        default="test_pagination_user",
        help="User ID for test campaign",
    )
    args = parser.parse_args()

    run_start = datetime.now(timezone.utc)

    print("=" * 60)
    print("Story Pagination E2E Test")
    print("=" * 60)

    server_url = args.server_url
    local_server: LocalServer | None = None
    server_pid: int | None = None
    env_overrides: dict[str, str] = {}

    try:
        if args.start_local:
            print("\nStarting local Flask server (TESTING=true, MOCK_SERVICES_MODE=false)...")
            print("  - Real LLM API calls (Gemini)")
            print("  - Real Firestore database")
            print("  - Auth bypass enabled for testing")
            port = pick_free_port()
            env_overrides: dict[str, str] = {
                "MOCK_SERVICES_MODE": "false",
                "TESTING": "true",
            }
            # Capture credential env vars for evidence provenance
            for cred_var in ["WORLDAI_GOOGLE_APPLICATION_CREDENTIALS", "GOOGLE_APPLICATION_CREDENTIALS", "FIRESTORE_EMULATOR_HOST"]:
                if os.environ.get(cred_var):
                    env_overrides[cred_var] = os.environ[cred_var]
            local_server = start_flask_server(port)
            server_url = local_server.base_url
            server_pid = local_server.proc.pid
            print(f"  Server URL: {server_url}")
            print(f"  Server PID: {server_pid}")
            print(f"  Log file: {local_server.log_path}")

            print("  Waiting for server to be ready...")
            if not wait_for_server(server_url, timeout=90):  # Flask may take longer
                print("  ERROR: Server did not start in time")
                print(f"  Check log file: {local_server.log_path}")
                return 1
            print("  Server ready!")

        # Verify real mode
        print(f"\nVerifying real mode at {server_url}...")
        if not verify_real_mode(server_url):
            print("ERROR: Cannot connect to server or server is not in real mode")
            return 1
        print("  Real mode verified!")

        # Run tests
        results, ctx = run_tests(server_url, args.user_id)

        # Summary
        print("\n" + "=" * 60)
        print("RESULTS SUMMARY")
        print("=" * 60)

        passed = sum(1 for r in results if r.passed)
        total = len(results)

        for r in results:
            status = "PASS" if r.passed else "FAIL"
            print(f"  [{status}] {r.name}: {r.details[:60]}")

        print("=" * 60)
        print(f"TOTAL: {passed}/{total} PASSED")
        print("=" * 60)

        # Save evidence if requested
        if args.savetmp:
            print("\nSaving evidence...")
            # Use /tmp/<repo>/<branch>/<work_name>/<timestamp>/ path per evidence-standards.md
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            evidence_base = get_evidence_dir(args.work_name)
            evidence_path = evidence_base / timestamp
            evidence_path.mkdir(parents=True, exist_ok=True)

            # Capture provenance using evidence_utils
            provenance = capture_provenance(
                server_url,
                server_pid=server_pid,
                server_env_overrides=env_overrides,
            )
            print(f"  Git branch: {provenance.get('git_branch', 'N/A')}")
            print(f"  Git HEAD: {provenance.get('git_head', 'N/A')[:12] if provenance.get('git_head') else 'N/A'}")

            # Convert TestResult objects to dicts for evidence bundle
            scenarios = [
                {
                    "name": r.name,
                    "passed": r.passed,
                    "details": r.details,
                    "errors": [] if r.passed else [r.details],
                    "data": r.data,
                }
                for r in results
            ]
            results_dict = {
                "server_url": server_url,
                "run_start": run_start.isoformat(),
                "campaign_id": ctx.campaign_id,
                "user_id": ctx.user_id,
                "scenarios": scenarios,
            }

            # Create evidence bundle per evidence-standards.md
            # Use get_captures_as_dict() for JSON-serializable request/response captures
            request_responses = None
            if hasattr(ctx.client, "get_captures_as_dict"):
                request_responses = ctx.client.get_captures_as_dict()

            bundle_files = create_evidence_bundle(
                evidence_path,
                test_name=args.work_name,
                provenance=provenance,
                results=results_dict,
                request_responses=request_responses,
                server_log_path=local_server.log_path if local_server else None,
            )

            # Save raw pagination captures as separate evidence file
            if ctx.pagination_captures:
                pagination_captures_data = [
                    {
                        "endpoint": cap.endpoint,
                        "params": cap.params,
                        "timestamp": cap.timestamp,
                        "entry_ids": cap.entry_ids,
                        "entry_timestamps": cap.entry_timestamps,
                        "pagination": cap.response.get("pagination", {}),
                        "story_count": len(cap.response.get("story", [])),
                    }
                    for cap in ctx.pagination_captures
                ]
                pagination_file = evidence_path / "pagination_responses.jsonl"
                lines = [json.dumps(c) for c in pagination_captures_data]
                write_with_checksum(pagination_file, "\n".join(lines) + "\n")
                bundle_files["pagination_responses"] = pagination_file
                print(f"  Pagination captures: {len(ctx.pagination_captures)} API calls")

            print(f"  Evidence saved to: {evidence_path}")
            print(f"  Bundle files: {list(bundle_files.keys())}")

        return 0 if passed == total else 1

    finally:
        if local_server:
            print("\nStopping local server...")
            local_server.stop()


if __name__ == "__main__":
    sys.exit(main())
